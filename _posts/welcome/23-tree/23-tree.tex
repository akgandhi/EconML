\documentclass[mathserif]{beamer}

\setbeamertemplate{frametitle}[default][center]%Centers the frame title.
\setbeamertemplate{navigation symbols}{}%Removes navigation symbols.
\setbeamertemplate{footline}{\raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[10pt]{\scriptsize\insertframenumber}}}}

\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{nicefrac}
\usepackage{graphicx,array,dsfont}
\usepackage{wrapfig}
\usepackage{harvard}
\usepackage{multirow}
\citationmode{abbr}

\newcommand{\Hrule}{\rule{\linewidth}{0.2pt}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\def\half{\frac{1}{2}}
\def\th{\mathrm{th}}
\def\sign{\mathrm{sign}}
\def\supp{\mathrm{supp}}
\def\E{\mathrm{E}}
\def\P{\mathrm{P}}
\def\Var{\mathrm{Var}}
\def\Cov{\mathrm{Cov}}
\def\Cor{\mathrm{Cor}}
\def\var{\mathrm{var}}
\def\cov{\mathrm{cov}}
\def\cor{\mathrm{cor}}
\def\rcor{\mathrm{rcor}}
\def\mcor{\mathrm{mcor}}
\def\mCor{\mathrm{mCor}}
\def\dCov{\mathrm{dCov}}
\def\dcov{\mathrm{dcov}}
\def\dVar{\mathrm{dVar}}
\def\dvar{\mathrm{dvar}}
\def\dCor{\mathrm{dCor}}
\def\dcor{\mathrm{dcor}}
\def\trace{\mathrm{trace}}
\def\col{\mathrm{col}}
\def\R{\mathds{R}} 
\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cN{\mathcal{N}}
\def\hbeta{\hat{\beta}}
\def\hy{\hat{y}}
\def\red{\color[rgb]{0.8,0,0}}
\def\white{\color[rgb]{1,1,1}}
\def\blue{\color[rgb]{0,0,0.8}}
\def\green{\color[rgb]{0,0.4,0}}

\begin{document}

\title{Tree-based methods for classification and regression}
\author{Ryan Tibshirani \\ Data Mining: 36-462/36-662}
\date{April 11 2013}

\begin{frame}
\maketitle
{\it Optional reading: ISL 8.1, ESL 9.2}
\end{frame} 

\begin{frame}
\frametitle{Tree-based methods}
{\red Tree-based based} methods for predicting $y$ from a feature
vector $x \in \R^p$ divide up the feature space into rectangles,
and then fit a very simple model in each rectangle. This works
both when $y$ is discrete and continuous, i.e., both for 
{\red classification} and {\red regression}

\bigskip
Rectangles can be achieved by making successive 
binary splits on the predictors variables $X_1,\ldots X_p$. I.e.,
we choose a variable $X_j$, $j=1,\ldots p$, {\red divide} up the 
feature space according to 
$$X_j \leq c \;\;\;\text{and}\;\;\; X_j > c$$ Then we proceed
on each half

\bigskip For simplicity, consider classification first
(regression later). If a half is ``pure'', meaning that it
mostly contains points from one class, then we
don't need to continue splitting; otherwise, we continue splitting
\end{frame}

\begin{frame}
\frametitle{Example: simple classification tree}
Example: $n=500$ points in $p=2$ dimensions, falling into classes 
0 and 1, as marked by colors

\smallskip
\begin{center}
\includegraphics[width=2in]{simpledata.pdf}
\end{center}

\vspace{-4pt}
Does dividing up the feature space into rectangles look like it
would work here?
\end{frame}

\begin{frame}
\frametitle{}
\bigskip
\includegraphics[width=\textwidth]{simpletree.pdf}
\end{frame}

\begin{frame}
\frametitle{Example: HCV treatment flow chart}
\smallskip
\begin{center}
\includegraphics[width=0.75\textwidth]{flowchart.jpg}
\end{center}
(From {\footnotesize \url{http://hcv.org.nz/wordpress/?tag=treatment-flow-chart}})
\end{frame}

\begin{frame}
\frametitle{Classification trees}
\smallskip
{\red Classification trees} are popular because they are 
interpretable, and maybe also because they mimic the way (some) decisions 
are made

\bigskip
Let $(x_i,y_i)$, $i=1,\ldots n$ be the training data, where 
$y_i \in \{1,\ldots K\}$ are the class labels, and 
$x_i \in \R^p$
measure the $p$ predictor variables. The classification tree can
be thought of as defining $m$ regions (rectangles) $R_1,\ldots R_m$, each 
corresponding to a leaf of the tree

\bigskip
We assign each $R_j$ a class label $c_j \in \{1,\ldots K\}$. We then 
classify a new point $x \in \R^p$ by
$$\hat{f}^\mathrm{tree}(x) \;=\; \sum_{j=1}^m c_j \cdot 1\{x \in R_j\}
\;=\; c_j \;\, \mathrm{such}\;\mathrm{that}\;\, x \in R_j$$
Finding out which region a given point $x$ belongs to is {\red easy} since
the regions $R_j$ are defined by a tree---we just scan down the tree.
Otherwise, it would be a lot harder (need to look at each region)
\end{frame}

\begin{frame}
\frametitle{Example: regions defined by a tree}
\bigskip
\includegraphics[width=2.2in]{ex1-tree.png}
\includegraphics[width=2.2in]{ex1-regions.png}

\bigskip
(From ESL page 306)
\end{frame}

\begin{frame}
\frametitle{Example: regions not defined a tree}
\begin{center}
\bigskip
\includegraphics[width=2.5in]{ex1-notpos.png}
\end{center}
\vspace{-5pt}
(From ESL page 306)
\end{frame}

\begin{frame}
\frametitle{Predicted class probabilities}
\smallskip
\smallskip
With classification trees, we can also get not only the predicted
classes for new points but also the {\red predicted class probabilties}

\bigskip
Note that each region $R_j$ contains some subset of the training data
$(x_i,y_i)$, $i=1,\ldots n$, say, $n_j$ points. The predicted class $c_j$ is 
just most common occuring class among these points. Further, for each class
$k=1,\ldots K$, we can estimate the probability that the class label is $k$
given that the feature vector lies in region $R_j$, $\P(C=k|X\in R_j)$, by
$$\hat{p}_k (R_j) = \frac{1}{n_j}\sum_{x_i \in R_j} 1\{y_i = k\}$$
the {\red proportion of points} in the region that are of class $k$. 
We can now express the predicted class as
$$c_j = \argmax_{k=1,\ldots K} \; \hat{p}_k (R_j)$$
\end{frame}

\begin{frame}
\frametitle{Trees provide a good balance}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
& \parbox{0.8in}{\centering Model \\ assumptions?} &
\parbox{0.8in}{\centering Estimated \\ probabilities?} &
Interpretable? & Flexible? \\
\hline
LDA & Yes & Yes & Yes & No \\
\hline
LR & Yes & Yes & Yes & No \\
\hline
$k$-NN & No & No & No & Yes \\
\hline
Trees & No & Yes & Yes & Somewhat \\
\hline
\end{tabular}

\bigskip
\bigskip
\begin{tabular}{|c|c|}
\hline
& \parbox{1in}{\centering Predicts \\ well?} \\
\hline
LDA & Depends on $X$ \\
\hline
LR & Depends on $X$ \\
\hline
$k$-NN & \parbox{1in}{\centering If properly \\ tuned} \\
\hline
Trees & ? \\
\hline
\end{tabular}
\end{center}

%% \smallskip
%% Remember that for both {\red LDA} and {\red logistic regression} we were able 
%% to estimate the class probabilities $P(C=j|X=x)$; for LDA we did this by
%% assuming that $X|C=j$ had a Gaussian distribution, for logistic regression
%% we did this by assuming that the log odds was linear in $x$. These methods
%% were also nice in that they could (potentially) give insight into the relationship 
%% between $X$ and $C$.

%% \bigskip
%% For the {\red model-free} classification methods, by $k$-nearest-neighbors 
%% and $K$-means clustering, there wasn't obvious way to estimate the class
%% probabilities. (You could think of a way to do this for large numbers
%% of neighbors of large numbers of centroids---but still, these wouldn't
%% be great.) The methods were also not interpretable.

%% \bigskip
%% Trees provide a nice {\red balance} between the two types of rules
%% above. They don't make any model assumptions, but still are highly interpretable 
%% and give estimated class probabilities. Their decision boundaries (rectangles) aren't 
%% as flexible as those for $k$-nearest- neighbors or $K$-means clustering, but are
%% still pretty general.
\end{frame}

\begin{frame}
\frametitle{How to build trees?}

\smallskip
There are two main issues to consider in building a tree:
\begin{enumerate}
\item How to choose the splits?
\item How big to grow the tree?
\end{enumerate}

\bigskip
Think first about varying the depth of the tree ... which is more complex,
a big tree or a small tree? What {\red tradeoff} is at play here? 
How might we eventually consider choosing the depth?

\bigskip
Now for a fixed depth, consider choosing the splits. If the tree has 
depth $d$ (and is balanced), then it has $\approx 2^d$ nodes. At each node
we could choose any of $p$ the variables for the split---this means 
that the number of possibilities is
\vspace{-3pt}
$$p \cdot 2^d
\vspace{-3pt}$$
This is {\red huge} even for moderate $d$! And we haven't even
counted the actual split points themselves
\end{frame}

\begin{frame}
\frametitle{The CART algorithm}
\smallskip
The {\red CART algorithm}\footnote{Breiman et al. (1984), ``Classification
and Regression Trees''} 
chooses the splits in a top down fashion: then chooses the first variable 
to at the root, then the variables at the second level, etc. 

\bigskip
At each stage we choose the split to achieve the biggest
drop in misclassification error---this is called a {\red greedy} strategy.
In terms of tree depth, the strategy is to grow a large tree and then {\red prune} 
at the end

\smallskip
\smallskip
\begin{tabular}{cc}
\parbox{0.5\textwidth}{
Why grow a large tree and prune, instead of just stopping at some point?
Because any stopping rule may be {\red short-sighted}, in that a split
may look bad but it may lead to a good split below it} &
\parbox{0.4\textwidth}{
\includegraphics[width=0.4\textwidth]{stop.pdf}}
\end{tabular}
\end{frame}

\begin{frame}
\frametitle{}
Recall that in a region $R_m$, the proportion of points in class $k$ is
$$\hat{p}_k(R_m) = \frac{1}{n_m} \sum_{x_i \in R_m} 1\{y_i=k\}.$$
The CART algorithm begins by considering splitting on variable $j$ and split 
point $s$, and defines the regions
$$R_1 = \{X \in \R^p : X_j \leq s\}, \;\;
R_2 = \{X \in \R^p : X_j > s\}$$
We then {\red greedily} chooses $j,s$ by minimizing the misclassification error
$$\argmin_{j,s} \; \Big( \big[1-\hat{p}_{c_1}(R_1)\big] + 
\big[1-\hat{p}_{c_2}(R_2)\big] \Big)$$
Here $c_1 = \argmax_{k=1,\ldots K} \, \hat{p}_k(R_1)$ is the most common class
in $R_1$, and $c_2 = \argmax_{k=1,\ldots K} \, \hat{p}_k(R_2)$ is the most 
common class in $R_2$
\end{frame}

\begin{frame}
\frametitle{}
\smallskip
\bigskip
Having done this, we now repeat this within each of the newly defined
regions $R_1,R_2$. That is, it again considers splitting all variables $j$ and
split points $s$,  within each of $R_1,R_2$, this time greedily choosing the pair 
that provides us with the biggest {\red improvement} in misclassification error

\bigskip
How do we find the best split $s$? Aren't there {\red infinitely many} to consider?
No, to split a region $R_m$ on a variable $j$, we really only need to consider $n_m$ 
splits (or $n_m-1$ splits)

\smallskip
\begin{center}
\includegraphics[width=1.5in]{split1.pdf}
\includegraphics[width=1.5in]{split2.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{}
\bigskip
\begin{tabular}{cc}
\parbox{0.35\textwidth}{
\includegraphics[width=0.35\textwidth]{ex1-tree.png}} &
\hspace{3pt}
\parbox{0.5\textwidth}{
Continuing on in this manner, we will get a big tree $T_0$. Its leaves define regions
$R_1,\ldots R_m$. We then {\red prune} this tree, meaning that we collapse some of its 
leaves into the parent nodes}
\end{tabular}

\smallskip
\smallskip
For any tree $T$, let $|T|$ denote its number of leaves.
We define
\vspace{-3pt}
$$C_\alpha(T) = \sum_{j=1}^{|T|} \big[1-\hat{p}_{c_j}(R_j)\big] + \alpha|T|
\vspace{-3pt}$$
We seek the tree $T \subseteq T_0$ that minimizes $C_\alpha(T)$. It turns out that 
this can be done by pruning the weakest leaf one at a time. Note that $\alpha$ is 
a {\red tuning parameter}, and a larger $\alpha$ yields a smaller tree.
CART picks $\alpha$ by 5- or 10-fold cross-validation
\end{frame}

\begin{frame}
\frametitle{Example: simple classification tree}
\smallskip
Example: $n=500$, $p=2$, and $K=2$. We ran CART:

\smallskip
\smallskip
\includegraphics[width=\textwidth]{simpletree.pdf}

\smallskip
\smallskip
To use CART in R, you can use either of the functions {\tt rpart} or {\tt tree}, in the 
packages of those same names.
When you call {\tt rpart}, cross-validation is performed automatically; 
when you call {\tt tree}, you must then call {\tt cv.tree} for cross-validation
\end{frame}

\begin{frame}
\frametitle{Example: spam data}
\smallskip
Example: $n=4601$ emails, of which 1813 are considered spam. For each email
we have $p=58$ attributes. The first 54 measure the frequencies of 54 key words
or characters (e.g., ``free'', ``need'', ``\$''). The last 3 measure 
\begin{itemize}
\item the average length of uninterrupted sequences of capitals;
\item the length of the longest uninterrupted sequence of capitals;
\item the sum of lengths of uninterrupted sequences of capitals
\end{itemize}
(Data from ESL section 9.2.5)

\bigskip
An aside: how would we possibly get thousands of emails labeled as spam or not?
\begin{center}
\includegraphics[width=0.6\textwidth]{gmail.png}
\end{center}
This is great! Every time you label an email as spam, gmail has more training datafg
\end{frame}

\begin{frame}
\frametitle{}
\bigskip
Cross-validation error curve for the spam data (from ESL page 314):

\vspace{-5pt}
\begin{center}
\includegraphics[width=3.5in]{spamcv.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{}
\bigskip
Tree of size 17, chosen by cross-validation (from ESL page 315):

\vspace{-5pt}
\begin{center}
\includegraphics[width=2.7in]{spamtree.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Other impurity measures}
We used misclassification error as a measure of the impurity of region $R_j$,
$$1-\hat{p}_{c_j}(R_j)$$
But there are other useful measures too: the {\red Gini index}:
$$\sum_{k=1}^K \hat{p}_k(R_j) \big[1-\hat{p}_k(R_j)\big],$$
and the {\red cross-entropy} or {\red deviance}:
$$-\sum_{k=1}^K \hat{p}_k(R_j) \log\big\{\hat{p}_k(R_j) \big\}.$$
Using these measures instead of misclassification error is sometimes preferable 
because they are more sensitive to changes in class probabilities. Overall, 
they are all pretty similar (Homework 6)
\end{frame}

\begin{frame}
\frametitle{Regression trees}
Suppose that now we want to predict a {\red continuous} outcome instead of a class
label. Essentially, everything follows as before, but now we just fit a constant 
inside each rectangle

\smallskip
\begin{center}
\includegraphics[width=2in]{ex1-tree.png}
\includegraphics[width=2in]{regtree.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{}
\smallskip
The estimated regression function has the form
$$\hat{f}^\mathrm{tree}(x) \;=\; \sum_{j=1}^m c_j \cdot 1\{x \in R_j\}
\;=\; c_j \;\, \mathrm{such}\;\mathrm{that}\;\, x \in R_j$$
just as it did with classification. The quantities $c_j$ are
no longer predicted classes, but instead they are real numbers. 
How would we choose these? Simple: just take the average response of all of the
points in the region,
$$c_j = \frac{1}{n_j} \sum_{x_i \in R_j} y_i$$ 

The main difference in building the tree is that we use {\red squared error loss} 
instead of misclassification error (or Gini index or deviance) to decide which region
to split. Also, with squared error loss, choosing $c_j$ as above is optimal
\end{frame}

\begin{frame}
\frametitle{How well do trees predict?}
\smallskip
Trees seem to have a lot of things going in the favor. So how is their 
predictive ability?

\bigskip
Unfortunately, the answer is {\red not great}. Of course, at a high level, 
the prediction error is governed by bias and variance, which in turn
have some relationship with the size of the tree (number of nodes). 
A larger size means smaller bias and higher variance, and a smaller
tree means larger bias and smaller variance

\bigskip
But trees generally suffer from high variance because they are quite
{\red instable}: a smaller change in the observed data can lead to a 
dramatically different sequence of splits, and hence a different prediction.
This instability comes from their hierarchical nature; once a split is made,
it is permanent and can never be ``unmade'' further down in the tree

\bigskip
We'll learn some variations of trees have {\red much better} predictive
abilities. However, their predictions rules aren't as transparent
\end{frame}

\begin{frame}
\frametitle{Recap: trees for classification and regression}
In this lecture, we learned about {\red trees} for classification and 
regression. Using trees, we divide the feature space up into rectangles
by making successive splits on different variables, and then within
each rectangle (leaf of the tree), the predictive task is greatly 
{\red simplified}. I.e., in classification, we just predict the most
commonly occuring class, and in regression, we just take the average
response value of points in the region

\bigskip
The space of possible trees is huge, but we can fit a good tree using
a {\red greedy} strategy, as is done by the {\red CART algorithm}.
It also grows a large tree, and then prunes back at the end, choosing
how much to prune by cross-validation

\bigskip
Trees are model-free and are easy to interpret, but generally speaking,
aren't very powerful in terms of predictive ability. Next time we'll learn
some procedures that use trees to make excellent prediction engines (but
in a way we lose interpretability)
\end{frame}


\begin{frame}
\frametitle{Next time: bagging}
\smallskip
Fitting small trees on bootstrapped data sets, and averaging predictions
at the end, can greatly reduce the prediction error (from ESL page 285):

\begin{center}
\includegraphics[width=2.5in]{bagging.png}
\end{center}
\end{frame}

\end{document}


