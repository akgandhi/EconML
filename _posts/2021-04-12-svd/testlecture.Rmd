---
title: "SVD and PCA"
author: "Amit Gandhi"
date: "10/29/2019"
output:
  slidy_presentation: default
  beamer_presentation: default
  ioslides_presentation: default
header-includes:
- \usepackage{tikz, amsmath, geometry}
- \usetikzlibrary{matrix,positioning,decorations.pathreplacing}
- \DeclareMathOperator{\Mcol}{col}
- \DeclareMathOperator{\Mrow}{row}
- \DeclareMathOperator{\Mnull}{null}
---

# SVD

## Matrices and Linear Transformations
A matrix $A \in \mathbb{R}^{n \times m}$ can be thought of as
a linear mapping between two spaces:
\begin{align*}
A: \mathbb{R}^m \rightarrow \mathbb{R}^n
\end{align*}
This interpretation requires no assumptions on the shape or
structure of the matrix $A$.

## Singular Value Decomposition
The singular value decomposition writes the matrix $A$ as a product
of three matricies:
\begin{align*}
A &= U \Sigma V^t
\end{align*}
Where $U \in \mathbb{R}^{n\times n}$ and $V \in \mathbb{R}^{m\times m}$
are orthonormal matricies and $\Sigma$ is the rectangular diagonal matrix
$\text{diag}(\sigma_1, \sigma_2, \ldots, \sigma_{\text{min}(n,m)})$.

\pause This decompositon exists for any real matrix $A$.


## Singluar Values
By convention, the values of $\Sigma$ are arranged in decending
order: $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{\text{min}(n,m)}$.

\pause These are called the \textbf{singular values} of the matrix $A$.

\pause The number of non-zero singular values is equal to the rank of the
matrix $A$.

## SVD Visual
\begin{align*}
A &= U\Sigma V^{T} \\
&=
 \resizebox {\textwidth} {!} {
\begin{tikzpicture}[
baseline,
mymat/.style={
  matrix of math nodes,
  ampersand replacement=\&,
  left delimiter=(,
  right delimiter=),
  nodes in empty cells,
  nodes={outer sep=-\pgflinewidth,text depth=0.5ex,text height=2ex,text width=1.2em}
}
]
\begin{scope}[every right delimiter/.style={xshift=-3ex}]
\matrix[mymat] (matu)
{
 \& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
};
\node 
  at ([shift={(3pt,-7pt)}]matu-3-2.west) 
  {$\cdots$};
\node 
  at ([shift={(3pt,-7pt)}]matu-3-5.west) 
  {$\cdots$};
\foreach \Columna/\Valor in {1/1,3/r,4/{r+1},6/m}
{
\draw 
  (matu-1-\Columna.north west)
    rectangle
  ([xshift=4pt]matu-6-\Columna.south west);
\node[above] 
  at ([xshift=2pt]matu-1-\Columna.north west) 
  {$u_{\Valor}$};
}
\draw[decorate,decoration={brace,mirror,raise=3pt}] 
  (matu-6-1.south west) -- 
   node[below=4pt] {$\Mcol(A)$}
  ([xshift=4pt]matu-6-3.south west);
\draw[decorate,decoration={brace,mirror,raise=3pt}] 
  (matu-6-4.south west) -- 
   node[below=4pt] {$\Mnull(A)$}
  ([xshift=4pt]matu-6-6.south west);
\end{scope}
\matrix[mymat,right=10pt of matu] (matsigma)
{
\sigma_{1} \& \& \& \& \& \\
\& \ddots \& \& \& \& \\
\& \& \sigma_{r} \& \& \& \\
\& \& \& 0 \& \& \\
\& \& \& \& \ddots \& \\
\& \& \& \& \& 0 \\
};
%\begin{scope}[every right delimiter/.style={xshift=-3ex}]
\matrix[mymat,right=25pt of matsigma] (matv)
{
 \& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
};
\foreach \Fila/\Valor in {1/1,3/r,4/{r+1},6/n}
{
\draw 
  ([yshift=-6pt]matv-\Fila-1.north west)
    rectangle
  ([yshift=-10pt]matv-\Fila-6.north east);
\node[right=12pt] 
  at ([yshift=-8pt]matv-\Fila-6.north east) 
  {$v^{T}_{\Valor}$};
}
\draw[decorate,decoration={brace,raise=37pt}] 
  ([yshift=-6pt]matv-1-6.north east) -- 
   node[right=38pt] {$\Mrow(A)$}
  ([yshift=-10pt]matv-3-6.north east);
\draw[decorate,decoration={brace,raise=37pt}] 
  ([yshift=-6pt]matv-4-6.north east) -- 
   node[right=38pt] {$\Mnull(A)$}
  ([yshift=-10pt]matv-6-6.north east);
\end{tikzpicture}
}
\end{align*}

## Rank-1 Matrix Decomposition
The singular value decomposition allows us to write the matrix $A$ as a sum
of $r$, rank $1$ matricies:
\begin{align*}
A &= \sum_i^{r=\text{rank}(A)} \sigma_i u_i v_i^t
\end{align*}

## Why these basis vectors?
A useful way of viewing the singular value decomposition is to think about
what would happen when projecting columns of $U$ and $V$:
\begin{align*}
A v_i &= \sigma_i u_i \\
A^t u_i &= \sigma_i v_i
\end{align*}
\pause Notice that both equations use $\sigma_i$!

## Diagnonalizing $A$
Therefore, if we have an arbitrary vector $z \in \mathbb{R}^m$
and we write it in the basis of $V$:
\begin{align*}
z &= \sum_i \alpha_i v_i
\end{align*}
The mapping of $A$ can be easily calculated in the coordinate system
of $U$:
\begin{align*}
Az &= \sum_i \alpha_i \sigma_i u_i
\end{align*}
Due to the linearity of the matrix operation.

## A Picture
\begin{center}
\includegraphics[width=\textwidth]{svd_2d.png}
\end{center}

## Simple Example
```{r}
A <- matrix(1:6,ncol=3)
A

SVD <- svd(A, nu=2, nv=3)
Sigma <- cbind(diag(SVD$d),0)
U <- SVD$u
V <- SVD$v
A - U %*% Sigma %*% t(V)
```

## Unit Ball
```{r}
N <- 1e4
p <- 3
unitBall <- matrix(runif(N * p, -1, 1), nrow=3)
unitBall <- unitBall[,apply(unitBall^2, 2, sum) < 1]
unitBall[,1:4]
```

## Picture
```{r}
projUnitBall <- t(A %*% unitBall)
plot(projUnitBall,pch=".")
```

## Another Picture
```{r}
v1 <- (A %*% V)[,1]
v2 <- (A %*% V)[,2]
plot(projUnitBall,pch=".")
arrows(0,0,v1[1],v1[2],col="red",lwd=2)
arrows(0,0,v2[1],v2[2],col="green",lwd=2)
```

## Least Squares
The standard formulation of least squares projection involves the matrix inverse of $X^tX$:
\begin{align*}
 \widehat{\beta} &= (X^t X)^{-1} X^t y
\end{align*}

## Problem
Why might this be a problem? Well, consider the simple case where
we have $n = p = 2$ with the following:
\begin{align*}
X &= \left( \begin{array}{cc} 10^9 & -1 \\ -1 & 10^{-5} \end{array}\right) \\
\beta &= \left( \begin{array}{c} 1 \\ 1 \end{array}\right)
\end{align*}
\pause For simplicity, we'll even assume that there is no noise vector. Then
we have:
\begin{align*}
y &= \left( \begin{array}{cc} 10^9 & -1 \\ -1 & 10^{-5} \end{array}\right) * \left( \begin{array}{c} 1 \\ 1 \end{array}\right) \\
&= \left( \begin{array}{c} 10^9 - 1 \\ -0.99999 \end{array}\right)
\end{align*}

## What happens in R
```{r}
X  <- matrix(c(10^9, -1, -1, 10^(-5)), 2, 2)
beta <- c(1,1)
y <- X %*% beta
Xinv <- solve(X)
Xinv %*% y

```

## Alternatively
However, what if we try to calculate this with the normal
equations? Here we need to invert the matrix $X^t X$.
```{r, eval=FALSE}
XtXinv <- solve(t(X) %*% X)
```
\begin{verbatim}
Error in solve.default(t(X) %*% X) :
  system is computationally singular: reciprocal
  condition number = 8.09999e-23
\end{verbatim}
\pause R knows that this is not going to be good, and refuses to
calculate the inverse by default.

## What if we ignore the warning
Suppose that we turn off this warning (by setting the tolerance
to zero); what happens?
```{r}
XtXinv <- solve(t(X) %*% X, tol=0)
Xty <- t(X) %*% y
XtXinv %*% Xty
```

## What is happening
In a linear model, we only observe $X \beta$, rather than $\beta$ itself.
We have already seen that numerical problems can lead to multiple solutions
for which the $X\beta$'s is very similar but the regression vectors $\beta$
are quite different.

## What is happening
Say that we have an error (or noise) $\Delta$ in the term $\beta$. Consider the following quantity for a full rank matrix $A$:
\begin{align*}
\frac{|| A\delta ||_2}{||\delta||_2}
\end{align*}
\pause Let $\delta = \sum_i \alpha_i v_i$. Then:
\begin{align*}
\frac{|| A\delta ||_2}{||\delta||_2}
  &= \sqrt{\frac{\sum_i \sigma_i^2 \alpha_i^2 v_i^2}{\sum_i \alpha_i^2 v_i^2}}
\end{align*}
\pause We can see that the minimum occurs when $\delta$ is equal to $v_{\text{min}(n,m)}$.

\pause Likewise, the maximum occurs when $\delta$ is equal to $v_1$.

## What is happening
We  wish to control the ratio of the relative error in
estimation to that of projection: \pause
\begin{align*}
\frac{\text{rel. error estimation}}{\text{rel. error projection}}
 &= \frac{|| \beta + \Delta ||_2 / || \beta ||_2}{|| X(\beta + \Delta) ||_2 / || X\beta ||_2}
 < \epsilon
\end{align*}
\pause So we do not want large changes in $\Delta$ to yield relatively
small changes in the prediction space $X\beta$.

## Condition Number
Notice that we can re-arrange the equation as:
\begin{align*}
\frac{|| \beta + \Delta ||_2 / || X(\beta + \Delta) ||_2}{|| \beta ||_2 / || X\beta ||_2}
\end{align*}
And now we have an upper bound on the numerator and an lower bound on the
denominator via the singular values:
\begin{align*}
\frac{\text{rel. error estimation}}{\text{rel. error projection}} \leq \frac{\sigma_{max}}{\sigma_{min}}
\end{align*}
\pause This is called the \textit{condition number} of the matrix $A$, and was the
quantity R complained about when I tried to invert an ill-conditioned
matrix.

## SVD and the Normal Equations
If we take the SVD of the data matrix $X$, we have
\begin{align}
X &= U D V^t.
\end{align}
Plugging this into the ordinary least squares estimator gives:
\begin{align}
\beta &= (X^t X)^{-1} X^t y \\
&= (V D^t U^t U D V^t)^{-1} V D^t U^t y \\
&= (V D (U^t U) D V^t)^{-1} V D U^t y \\
&= (V D I_p D V^t)^{-1} V D U^t y \\
&= (V D^2 V^t)^{-1} V D U^t y
\end{align}

## Simplification
By taking the fact that a diagonal matrix is its own transpose and using that
$U^t U$ is equal to the identity. Note that $D^2$ is just a matrix with the
squared singular values along the diagonal.

Now, notice that the inverse of $V$ is $V^t$, and vice-versa. Further, the
inverse of $D^{2}$ is equal to a diagonal matrix with the inverse of the
squared singular values along the diagonal (this exists if we assume that
$\sigma_1 > 0$). Therefore:
\begin{align}
(V D^2 V^t)^{-1} &= (V^{t})^{-1} D^{-2} V^{-1} = V D^{-2} V^t
\end{align}

## Further Simplification
And we can further simplify the equation for the ordinary least squares
estimator:
\begin{align}
\beta &= (V D^2 V^t)^{-1} V D U^t y \\
&= V D^{-2} V^t V D U^t y \\
&= V D^{-2} D U^t y \\
&= V D^{-1} U^t y.
\end{align}
This gives us a compact way to write the ordinary least squares estimator.
It is also far more numerically stable to use this formula to compute the
estimate $\beta$ from a dataset. 

## Principal Component Analysis
The principal components of the matrix $X$ is a linear
reparameterization $T=XW$ of the matrix $X$ such that: \pause
-Each new coordinate is uncorrelated with the others; specifically,
W is an orthogonal matrix called the loadings \pause
-The first component has the largest variance of all
linear combinations of the columns of X, the second has the
highest variance conditioned on being uncorrelated with the
first, and so forth.

## PCA
Considering the first column of the matrix $W$, we can write the
condition as follows:
\begin{align*}
\arg\max_{w:\, ||w||_2 = 1} \left\{ ||Xw||_2 \right\}
\end{align*}
\pause However, we already know that this is maximized when
$w$ is a multiple of the first right singular vector. That is,
the first column of $V$ in the singular value decomposition $U\Sigma V^t$
of $X$.

## PCA
Likewise, we can argue that the second column of $W$ is the second
column of $V$, and so forth for all of the principal components.

\pause Therefore, the principal components are given by $T = XV$.
This gives:
\begin{align*}
T &= XV \\
&= U\Sigma V^t V \\
&= U \Sigma
\end{align*}
So the components are the weighted columns of the left singular vectors.


