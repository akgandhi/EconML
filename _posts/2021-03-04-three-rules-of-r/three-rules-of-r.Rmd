---
title: "Everything in R is an Object"
description: |
  The First Deep Truth of R
author:
  - name: Amit Gandhi
    url: {}
    
date: 03-04-2021
output:
  distill::distill_article:
    self_contained: false
preview: https://genialebooks.com/wp-content/uploads/2019/05/fd985c69272ef1d23a9ab3f19c918252-210x315.jpg
bibliography: bibliography.bib
---


```{r, include = FALSE }
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
```


# Introduction

- R is a rich software language for "programming with data" to quote the chief designer John Chambers [@chambers1998programming] of the precursor and inspiration for R - the S programming language. 

- While diverse in its applications, R is based on a few unifying design principles that allows one to separate the forest from the trees in developing R software for solving a problem. 

- A useful summary of these principles for understanding the behavior of R code is captured by the memorable [slogans](https://www.r-bloggers.com/three-deep-truths-about-r/):

1. 
>Everything in R is an object. 
>
> --- John Chambers

2. 
>Everything that happens in R is the result of a function call. 
>
> --- John Chambers

3. 
>Names have objects, objects don't have names. 
>
> --- Hadley Wickham


- We will refer to these principles as the *deep truths* of R. 
- Although these principles are omnipresent in R, they are reaffirmed and take center stage in the jump from "base R" (the original, core R language) to "modern R" as formalized by the **Tidyverse** ecosystem of packages.^[The tidyverse is a standardization and formalization of functional and meta-programming capabilities of R. The popularization and resurgence of R through the tidyverse owes an unusual amount to the influential packages and writings of Hadley Wickham - the author of *Advanced R* and co-author of *R for Data Science*.] 

- The first of these principles is arguably the most important to initiate an understanding of the full trilogy, and we begin our discussion there. 

# Everything in R is an Object


- Every entity that gets used in R is an object of some kind, and all computations involve constructing and manipulating objects in some form. 

- However, the word "object" has many connotations in computer programming, particularly in relation to "object-oriented programming". Hence it is crucial to recognize the more general meaning of objects in R. 


- "Everything in R is an object" in the sense of being a data structure that can be manipulated and analyzed. As @chambers1998programming writes, an object in R is a "dynamically created, self-describing container for data".


- When complemented with the fact that R is a language designed to "program with data", this means that everything in R is analyzable as a data. This is arguably what fundamentally distinguishes R from other languages - it achieves the capabilities of general purpose programming language through a paradigm that is centered around data structures and analysis of those structures. 

- To unpack the Chambers defintion of R objects, the two distinct ideas he raises are "containers of data" and "self-describing".

- These map directly to the **intrinsic attributes** that are associated to every R object.  Every object in R has (1) a *mode* or *type* and (2) a *length*. These are **intrinsic attributes** of every R object. 


```{r}
mode(1:10) #mode
typeof(1:10) #type
length(1:10) #length
```

- The fact every object has a length means they are vector-like in some fashion and can be approached as a data structure, e.g., we can in principle access, manipulate, and analyze any R object through common data techniques, e.g., the data underlying an object  `x` can be examined via a subsetting operation `x[[i]]` for `i` from `1:length(x)`. 

- While this kind of data analysis and manipulation is immediate and natural in the case of numerical vectors, it can more novel and revealing when applied to different R objects.


```{r}
typeof(quote(x+y))
length(quote(x+y))

cat("\n")

for (i in seq_along(quote(x+y))) {
  ex <- quote(x+y)
  cat(ex[[i]], " ", typeof(ex[[i]]), "\n" )
}
```


Not all objects subsetted like vectors despite being data containers with a `length`. `function` objects are notorious in this respect:


```{r}
#What is a function
f <- function() print("Hello World")
typeof(f)
length(f)
```
```{r, eval=FALSE}
f[[1]]
Error in f[[1]] : object of type 'closure' is not subsettable
```


We can however extract the underlying data via of a function object "as a container of data" through alternative function calls:


```{r}
formals(f)
body(f)
environment(f)
```

In addition to being containers of data, R objects are "self describing" in the Chambers definition. The key challenge with data structures of any form is to have a guide for interpreting what the underlying data mean. 

-The first clue as to the data meaning is the "base type" as given by `typeof()`.



- Of course, the interpretation of this data depend on the object type. The mode or type separates (1) data objects from (2) language objects. The data objects have modes that include `numeric` `integer`, `character`, `logical`, etc, whereas the language objects include `function`, `call`, `expression`, etc. 

Whereas the *mode* attribute was central to R's precursor language S, object types are a more common to distinguish different families of objects. The type of an object we will reference as its *base type* (to distinguish from what we will later discuss as the object's "class", which is an enrichment of the object's base type).

There are a fixed set of 25 types defined by core-R. They are listed below, categorized by their distinct data interpretation.

*   Vectors include types `NULL`, 
    `logical`, `integer`, `double`, `complex`, `character`, `list`, and `raw`.

    
    ```{r}
    typeof(NULL)
    typeof(1L)
    typeof(1i)
    ```


*   Functions include types `closure` (regular R 
    functions), `special` (internal functions), and 
    `builtin` (primitive functions).
    
    ```{r}
    typeof(mean)
    typeof(`[`)
    typeof(sum)    
    ```
    
    

*   Environments have type `environment`.

    ```{r}
    typeof(globalenv())
    ```

*   The `S4` type is used for S4 classes that 
    don't inherit from an existing base type.
   
    ```{r}
    mle_obj <- stats4::mle(function(x = 1) (x - 2) ^ 2)
    typeof(mle_obj)
    ```

*   Language components include `symbol` (aka 
    name), `language` (usually called calls), and 
    `pairlist` (used for function arguments) types.

    ```{r}
    typeof(quote(a))
    typeof(quote(a + 1))
    typeof(formals(mean))
    ```
 
    `expression` is a special purpose type that's only returned by
    `parse()` and `expression()`. Expressions are generally not needed in user 
    code.
 
*   The remaining types are esoteric and rarely seen in R. They are important 
    primarily for connections to C code: `externalptr`, `weakref`, 
    `bytecode`, `promise`, `...`, and 
    `any`.
    

All other attributes of an R object beyond its base type are a set of metadata associated with the object. This metadata is attached to the main object but subordinate in status. These attributes can be found en masse through a call to `attributes()`


```{r}
attributes(1:10)
```

- In this case the object has no additional attributes beyond its intrinsic attributes and hence the call returns a `NULL` value. 

- More generally, attributes are represented as a named list in R and can have value `NULL`


```{r}
x <- matrix(1:9, nrow = 3)
attributes(x)
typeof(attributes(x))
```


- An attribute that is often helpful for interpreting an object's data is the object's `class`. The `class` attribute is used by R's native object oriented system - the S3 system. Not all objects have the `class` attribute, and those that do we can are designated "OO objects" (short for "object-oriented"). This can be confusing in lieu of the fact (and discussed above) that everything in R is an object! 

- However as Wikham writes (*Advanced R*, page 291), "while everything is an object, not everything is object-oriented.". 
Hence the objects in R will come in two flavors that we can designate *base objects* and *OO objects*. 


![](oo-venn.png)

- The type of an object we will reference as its *base type* - this is because irrespective of whether an object is a base object or an OO object, its underlying type belongs to a pre-defined set established by R-core that was described above. 


- Hence even OO objects have a base type. What distinguishes OO types is that they will have at least an additional "class" attribute. The class attribute can then be seen as an extension of the functionality of its underlying base type. 

- As an aside - the S language, which is the basis for the core R language - was developed before the design of an OOP (Object Oriented Programming) system that introduced formal OO capabilities to the language. The original system - the S3 system - remains the simplest, most flexible, and most popular system used for R programming and package development. We focus attention on it here. Hence the simplest OO objects are S3 objects - e.g., a base type object with at least a `class` attribute.

To appreciate the S3 system, it is useful to observe that R is at heart a functional programming language, e.g., functions are **first-class functions**. This means that objects in R are functions just like any other object, e.g., they are just another data structure. It is not surprising then that the S3 system brings OOP capabilities through functions the thus adopts the OOP through a paradigm of *generic functions*, where the different methods for manipulating different classes of objects that achieve a common functionality are associated with a *generic* function. For example the `print()` function behaves differently

```{r}
some_data <- list(x = c(1,2), y = c(3,4))
print(some_data)
attributes(some_data)
class(some_data) <- c("data.frame")
attributes(some_data)$row.names = c(1L,2L)
some_data
## of course there are convenience functions for creating a data frame from a list that automate this process.
as.data.frame(list(x = c(1,2), y = c(3,4)))
```

How did this happen, e.g., the output of the `list` data structure change depending on the `class` attribute. The magic arises because the function `print` is actually a *generic*. As Hadley describes it (p. 300): "The generic is a middleman: its job is to define the interface (i.e., the arguments)  then find the right implementation for the job. The implementation for a specific class is called a **method**, and the generic finds that method by performing **method dispatch**. 

The fact that `print` is a generic is revealed by its body, which is a single line that calls `UseMethod`.

```{r}
body(print)
```

The `UseMethod` function inspects the `class` attribute of the first argument and seeks the appropriate method (function). The methods are named according to the *generic.class* convention. Hence printing the data frame above should seek a method by the name of `print.data.frame`. To check this behavior, we can use the helper function `sloop::s3_dispatch()`

```{r}
sloop::s3_dispatch(print(some_data))
```

The "=>" in the output indicates the method that is called, confirming our characterization of the method dispatch process. A more thorough examination of method dispatch is given in Chapter 13 of Advanced R. 

An object can have multiple classes, e.g., the class attribute takes the value of a character vector. For example, a *tibble* is an enhanced data frame, and has 3 classes. Lets convert our data into a tibble:

```{r}
class(some_data) <- append(c("tbl_df", "tbl"), class(some_data))
some_data
sloop::s3_dispatch(print(some_data))
```

In the case of objects with multiple classes, method dispatch first seeks a method for the first class, and then the second class, and so forth. 

One useful feature of tibbles is that they readily support *list columns*. Data frames and tibbles are lists of equal length vectors.

```{r echo=FALSE}
knitr::include_graphics("prog/images/list-columns/tibble.png")
```

The vectors contained inside of a data frame or tibble are typically _atomic vectors_. 

```{r echo=FALSE}
knitr::include_graphics("prog/images/list-columns/tibble-atomic.png")
```


However recall lists are also vectors (so called *recursive vectors* as discussed above). Thus if instead of an atomic vector, a tibble held a list as a column, this would give rise to (an appropriately called) _list column_


```{r echo=FALSE}
knitr::include_graphics(
  "prog/images/list-columns/tibble-list-col.png"
)
```

And because lists are recursive, the elements of a list column can be other lists, e.g, tibbles themselves!


```{r echo=FALSE}
knitr::include_graphics(
  "prog/images/list-columns/tibble-list-col-tibbles.png"
)
```

This enables some very powerful workflows with data and empirical modeling. As an example, suppose we want to break a large data set into multiple sub-data sets by split according to the values of a group variable, and run a regression among the remaining variables for each group separately. How can we examine the pattern of values of the regression coefficient among the groups. 

The key function to enable list columns that are populated with tibbles is `nest()`. In particular as Hadley explains, "Pass `nest()` the names of the columns to put into each individual tibble. `nest()` will create one row for each unique value of the remaining variables." (determine citation)

Consider the example from the `tidymodels` tutorials:


```{r}
library(tidymodels)

data(Orange)

Orange <- as_tibble(Orange)
Orange
```


There is clear correlation between age and circumference for each class of tree. 


```{r}
cor(Orange$age, Orange$circumference)
ggplot(Orange) + geom_line(aes(x = age, y = circumference, color = Tree))
```
```{r}
Orange %>% group_by(Tree) %>% summarize(correlation = cor(circumference,age)) %>% arrange(Tree)
```


Now if we wish to examine the statistical properties of the bivariate correlation for each group, the task becomes more complicated. We can examine a proper correlation test at the aggregate level



```{r}
cor_agg <- cor.test(Orange$age,Orange$circumference)
cor_agg
cor_agg %>% class()
```


The output takes the form of a rich collection of information that is a data structure of class `htest`. To convert this information into consumable data for analyses, we will "tidy" it (a running themse in the "R for Data Science" book and a key principle that unifies the package in the `tidyverse`.)

We tidy the data with the `tidy()` function:

```{r}
tidy(cor_agg)
```

which produces a single row (one observation - in this case a single test) with each variable/dimension of the test being a column. 

We can use this workflow to generate a tidy data set on the statistical properties of the correlation across the subgroups of our data. This is enabled via the `nest-map-unnest` pattern. 

First we nest the data to create a list column:

```{r}
Orange %>% nest(data = c(circumference, age))
```

We can add to the pipeline by mapping the correlation test across each tibble in the list column


```{r}
Orange %>% nest(data = c(circumference, age)) %>%
  mutate(test = map(data, ~cor.test(.$age, .$circumference)))
```

Notice this produces another list column consisting of elements of the `htest` class. Now we need to tidy each one, which will give us a list column of tibbles:


```{r}
Orange %>% nest(data = c(circumference, age)) %>%
  
  mutate(test = 
           map(data, ~cor.test(.$age, .$circumference))) %>%
  
  mutate(testdat = map(test,tidy))
```


If we inspect an element of the `testdat` variable, we will see a tidy data frame associated to the correlation test applied to a group of the data:

```{r}
(Orange %>% nest(data = c(circumference, age)) %>%
  
  mutate(test = 
           map(data, ~cor.test(.$age, .$circumference))) %>%
  
  mutate(testdat = map(test,tidy)))$testdat[[1]]
```
```{r}
cortestDat <-
Orange %>% nest(data = c(circumference, age)) %>%
  
  mutate(test = 
           map(data, ~cor.test(.$age, .$circumference))) %>%
  
  mutate(testdat = map(test,tidy)) %>%
  unnest(testdat) %>%
  select(-data, -test)
```
```{r}
ggplot(cortestDat) + geom_point(aes(x = p.value, y = estimate))
```

### S3 Atomic Vectors

Atomic vectors also have S3 variants. Base R includes definitions for several important S3 atomic vectors to represent:

- Categorical data using _factor_ vectors
- Dates using _Date_ vectors
- Date_times using _POSIXct_ vectors
- Durations using _difftime_ vectors


These S3 vectors (also called *augmented vectors* to signal their augmented attributes relative to their base vector type) and their connections to base type vectors are pictured below:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("adv-r/diagrams/vectors/summary-tree-s3-1.png")
```

Here we discuss factor vectors, which are built on top of integer vectors that can take one of a pre-defined set of values. A factor vector is defined on top of an integer vector through two additional attributes: (1): a `class` of "factor" which distinguishes factors from standard integer vectors, and (2): `levels` which are a set of allowed values. 

```{r}
x <- factor(c("a", "b,", "c", "d"))
x
typeof(x)
unclass(x)
```


R has functionality that allows you to immediately tabulate a factor vector based on the set of potential levels (as opposed to the observed levels)

```{r}
sex_char <- c("m", "m")
sex_factor <- factor(sex_char, levels = c("m", "f"))
table(sex_char)
table(sex_factor)
```

Ordered factors are a slightly different stripe of factors (an added class `ordered`) that orders the factor levels in a meaningful way that can be helpful for visualizations or modeling. We have already seen an example above in the case of the `Orange` data set:

```{r}
Orange$Tree
class(Orange$Tree)
```

which explains how `Tree` was arranged in the summary tables (based on this order).

When we import data we will generally want to convert character vectors in the data set into factors. The following workflow that we illustrate with a data set on European automobile sales and product characteristics leverages the `across()` functionality from `dplyr 1.0.0` to achieve these purposes and provides useful summaries. 

```{r}
carData <- read_csv("../../data/total.csv")
glimpse(carData)
```

Lets select which of the 90 variables are character columns:


```{r}
carData %>% select(where(is.character)) %>%
  glimpse()
```
```{r}
carData <- carData %>% mutate(across(c(year,month,time,muninumber,market_ids), as.factor)) %>% 
  mutate(across(where(is.character), as.factor))

carData %>% glimpse()
```



```{r}
carData %>%
  summarize(across(where(is.factor), nlevels))
```

How many years of data for each municpality


```{r}
carData %>%
group_by(muninumber, year) %>% count()
  #summarize(across(where(is.factor), nlevels))
```


Base R will tend to create factor vectors often because when data is read using `read.csv()`, character vectors are automatically coerced into factors. 


```{r}
carData <- read.csv("../../data/total.csv")
```

```{r}
library(readr)
carData2 <- read_csv("../../data/total.csv")
glimpse(carData)
```



