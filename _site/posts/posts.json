[
  {
    "path": "posts/2021-02-03-predictive-accuracy-the-what-and-the-how/",
    "title": "Predictive Accuracy: The What and the How",
    "description": "measuring and managing the performance of an algorithmic model",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-03",
    "categories": [],
    "contents": "\nIntroduction\nIn the last post, we examined decision tree as an algorithmic model to understand the behavior of data. As we saw, the optimal tree is one where we grew a large tree, and pruned it to avoid an “overfitting problem” as measured by the cost complexity criterion. But what is the over-fitting problem with algorithmic models, and why did the cost complexity criterion allow us to manage it?\nTrees are formed via an algorithm - CART - which is a predictive structure for predicting an outcome variable \\(Y\\) from explanatory variables \\(X\\). The particular tree that was formed depended on two key inputs\nThe training sample \\(D = \\{(y_1, x_1),\\dots, (y_n, x_n)\\}\\)\nThe tuning parameters (also called hyperparameters) \\(\\eta\\) which governs the behavior of the algorithm.\nThe predictive structure is connected to the conditional distribution of the outcome variable \\(Y\\) given the explanatory variables \\(X\\), e.g., a feature \\(F_{Y\\mid X}(y \\mid X = x)\\).\nGiven the training sample \\(D\\) and hyperparameters, the CART algorithm generates a fitted predictor \\(\\hat{m}\\). Although we do not have that much choice over the training sample \\(D\\) (although as we will see below, we have some choice), we do have more discretion over the tuning parameters. Thus a very practical problem we face in algorithmic model is that we must se tthe value of \\(\\eta\\), e.g., \\(\\hat{\\eta}\\). This process is called (rather simply) tuning. A more elaborate description would be hyperparameter optimization\nIn this post we examine the more general principles around tuning with an eye towards its practical implementation in the case of the CART algorithm.\nThe tuning problem\nWhat are the tuning parameters in CART? Lets take a look a look at the help file?\n\n\nrpart\n\n\nR Documentation\n\n\nRecursive Partitioning and Regression Trees\n\n\nDescription\n\n\nFit a rpart model\n\n\nUsage\n\n\nrpart(formula, data, weights, subset, na.action = na.rpart, method,\n      model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)\n\n\nArguments\n\n\nformula\n\n\n\na formula, with a response but no interaction terms. If this a a data frame, that is taken as the model frame (see model.frame).\n\n\n\ndata\n\n\n\nan optional data frame in which to interpret the variables named in the formula.\n\n\n\nweights\n\n\n\noptional case weights.\n\n\n\nsubset\n\n\n\noptional expression saying that only a subset of the rows of the data should be used in the fit.\n\n\n\nna.action\n\n\n\nthe default action deletes all observations for which y is missing, but keeps those in which one or more predictors are missing.\n\n\n\nmethod\n\n\n\none of “anova”, “poisson”, “class” or “exp”. If method is missing then the routine tries to make an intelligent guess. If y is a survival object, then method = “exp” is assumed, if y has 2 columns then method = “poisson” is assumed, if y is a factor then method = “class” is assumed, otherwise method = “anova” is assumed. It is wisest to specify the method directly, especially as more criteria may added to the function in future.\n\n\nAlternatively, method can be a list of functions named init, split and eval. Examples are given in the file ‘tests/usersplits.R’ in the sources, and in the vignettes ‘User Written Split Functions’.\n\n\n\nmodel\n\n\n\nif logical: keep a copy of the model frame in the result? If the input value for model is a model frame (likely from an earlier call to the rpart function), then this frame is used rather than constructing new data.\n\n\n\nx\n\n\n\nkeep a copy of the x matrix in the result.\n\n\n\ny\n\n\n\nkeep a copy of the dependent variable in the result. If missing and model is supplied this defaults to FALSE.\n\n\n\nparms\n\n\n\noptional parameters for the splitting function. Anova splitting has no parameters. Poisson splitting has a single parameter, the coefficient of variation of the prior distribution on the rates. The default value is 1. Exponential splitting has the same parameter as Poisson. For classification splitting, the list can contain any of: the vector of prior probabilities (component prior), the loss matrix (component loss) or the splitting index (component split). The priors must be positive and sum to 1. The loss matrix must have zeros on the diagonal and positive off-diagonal elements. The splitting index can be gini or information. The default priors are proportional to the data counts, the losses default to 1, and the split defaults to gini.\n\n\n\ncontrol\n\n\n\na list of options that control details of the rpart algorithm. See rpart.control.\n\n\n\ncost\n\n\n\na vector of non-negative costs, one for each variable in the model. Defaults to one for all variables. These are scalings to be applied when considering splits, so the improvement on splitting on a variable is divided by its cost in deciding which split to choose.\n\n\n\n…\n\n\n\narguments to rpart.control may also be specified in the call to rpart. They are checked against the list of valid arguments.\n\n\n\nDetails\n\n\nThis differs from the tree function in S mainly in its handling of surrogate variables. In most details it follows Breiman et. al (1984) quite closely. R package tree provides a re-implementation of tree.\n\n\nValue\n\n\nAn object of class rpart. See rpart.object.\n\n\nReferences\n\n\nBreiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) Classification and Regression Trees. Wadsworth.\n\n\nSee Also\n\n\nrpart.control, rpart.object, summary.rpart, print.rpart\n\n\nExamples\n\n\nfit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)\nfit2 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis,\n              parms = list(prior = c(.65,.35), split = \"information\"))\nfit3 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis,\n              control = rpart.control(cp = 0.05))\npar(mfrow = c(1,2), xpd = NA) # otherwise on some devices the text is clipped\nplot(fit)\ntext(fit, use.n = TRUE)\nplot(fit2)\ntext(fit2, use.n = TRUE)\n\n\nThe help file reveals several places where the algorithm can be tuned. Among them is a reference to an argument control, which is a list of options that control the detail of the algorithm. We can examine the parameters of the list:\n\n\nrpart.control\n\n\nR Documentation\n\n\nControl for Rpart Fits\n\n\nDescription\n\n\nVarious parameters that control aspects of the rpart fit.\n\n\nUsage\n\n\nrpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, \n              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,\n              surrogatestyle = 0, maxdepth = 30, ...)\n\n\nArguments\n\n\nminsplit\n\n\n\nthe minimum number of observations that must exist in a node in order for a split to be attempted.\n\n\n\nminbucket\n\n\n\nthe minimum number of observations in any terminal <leaf> node. If only one of minbucket or minsplit is specified, the code either sets minsplit to minbucket*3 or minbucket to minsplit/3, as appropriate.\n\n\n\ncp\n\n\n\ncomplexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. For instance, with anova splitting, this means that the overall R-squared must increase by cp at each step. The main role of this parameter is to save computing time by pruning off splits that are obviously not worthwhile. Essentially,the user informs the program that any split which does not improve the fit by cp will likely be pruned off by cross-validation, and that hence the program need not pursue it.\n\n\n\nmaxcompete\n\n\n\nthe number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n\n\n\nmaxsurrogate\n\n\n\nthe number of surrogate splits retained in the output. If this is set to zero the compute time will be reduced, since approximately half of the computational time (other than setup) is used in the search for surrogate splits.\n\n\n\nusesurrogate\n\n\n\nhow to use surrogates in the splitting process. 0 means display only; an observation with a missing value for the primary split rule is not sent further down the tree. 1 means use surrogates, in order, to split subjects missing the primary variable; if all surrogates are missing the observation is not split. For value 2 ,if all surrogates are missing, then send the observation in the majority direction. A value of 0 corresponds to the action of tree, and 2 to the recommendations of Breiman et.al (1984).\n\n\n\nxval\n\n\n\nnumber of cross-validations.\n\n\n\nsurrogatestyle\n\n\n\ncontrols the selection of a best surrogate. If set to 0 (default) the program uses the total number of correct classification for a potential surrogate variable, if set to 1 it uses the percent correct, calculated over the non-missing values of the surrogate. The first option more severely penalizes covariates with a large number of missing values.\n\n\n\nmaxdepth\n\n\n\nSet the maximum depth of any node of the final tree, with the root node counted as depth 0. Values greater than 30 rpart will give nonsense results on 32-bit machines.\n\n\n\n…\n\n\n\nmop up other arguments.\n\n\n\nValue\n\n\nA list containing the options.\n\n\nSee Also\n\n\nrpart\n\n\nWe can see a few tuning parameters that stand out - minsplit, minbucket, cp, and xval, which are initiatlized to specific values. The user has control of the algorithm by setting these values.\nPrediction Error\nThe tuning parameters in general should be set to maximize the performance of the model. But what is performance?\nSuppose we make a prediction \\(\\hat{Y}\\) and the realized outcome is \\(Y\\), where in general we should not expect perfect prediction and hence \\(\\hat{Y} \\neq Y\\). There is a loss \\(L(Y,\\hat{Y})\\) incurred from missing the target. Common loss functions are\n\\[\nL(\\hat{Y}, Y) = \n\\begin{cases}\n(Y-\\hat{Y})^2 \\mbox{ for regression}\\\\\n\\mathbf{1}(Y \\neq \\hat{Y}) \\mbox{ for classification}\n\\end{cases}\n\\]\nGiven the loss function, what is the predictive performance of the predictor \\(\\hat{m}\\).\nIf the goal is to use the predictor to predict in the population that has yet to be observed, then the natural performance criterion is the average loss in the population\n\\[\nErr_{D} = E_{Y^{0}, X^{0}} \\left[L(Y^{0}, \\hat{m}(X^{0})) \\mid D \\right]\n\\] where the expectation is taken with respect to the randomness in \\((Y^{0}, X^{0}) \\sim F_{Y,X}\\). This is the test error or generalization error, e.g., the average error we would expect in an independent test sample. Notice that the training sample \\(D\\) is conditioned out in the assesment of performance under \\(Err_{D}\\)\nAlthough \\(Err_{D}\\) is the ideal performance measure, it is challenging to analyze or forecast. However if we could measure it, then this provides a path to tuning. Notice that \\(Err_{D} = Err_{D}(\\eta)\\), and hence optimal tuning would amount to\n\\[\n\\hat{\\eta} = \\arg \\max_{\\eta} Err_{D}(\\eta)\n\\] Thus \\(Err_{D}(\\eta)\\) would serve two distinct purposes (ESL p 222)\n Model tuning : estimating the performance of different algorithmic models indexed by \\(\\eta\\) in order to choose the best one.\n Model assessment : having chosen a final model, estimating its prediction error (generalization error) on new data.\nIn an ideal circumstance to carry out these calculations we would have a sample of data broken into three parts.\nTraining Set\nValidation Set\nTest Set\n\n\n\nThe training set would be used to fit the data \\(\\hat{f}(\\eta)\\). The validation set could be used to tune the algorithm’s \\(\\eta\\) based on average loss in the validation set. And then the test set would provide a clean assesment of the test error of the tuned model at $.\nWe typically will not have enough data for this rather clean experiment without seriously compromising the size of the training sample (which detracts from the algorithm’s performance).\nInstead we will need to live in a second best world and consider a slightly augmented measure of performance, the average test error, where we also account for randomness in the training sample itself\n\\[\nErr(\\eta) = E_D\\left[Err_{D}\\right(\\eta)]\n\\] If we imagine ordering \\(\\eta\\) such that lower values of \\(eta\\) generate simpler fitted models, and higher values of \\(\\eta\\) generate more complex fitted models, then the behavior of \\(Err(\\eta)\\) creates the canonical picture of Bias Variance Tradeoff (ESL p. 220)\n\n\n\nOptimism\nLet the training sample be $ D = {(y_i,x_i)}_{i=1}^{n}$\nConsider a fitted model \\(\\hat{y}_{i}=\\hat{m}\\left(x_{i}\\right)\\), which can come from any algorithmic model \\(m\\), and implicitly depends on the tuning parameter \\(\\eta\\).\nFor simplicity consider a regression problem with squared error loss (the analysis below generalizes to many loss functions)\nThe in-sample training error is \\[\n\\overline{err}=\\frac{1}{n}\\sum\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n\\]\nWe have a general understanding that the \\(\\overline{err}\\) is an optimistic assessment of the object of interest, namely the test error \\(Err_{D}\\). Can we formalize the relationship to gauge just how optimistic?\nOne source of the discrepancy is that the test experiment is evaluated at potentially different points of the explanatory variables as compared to the training simple. To eliminate this additional variability, it is useful to consider the in-sample test error \\[\nErr_{\\mbox{in}} = \\frac{1}{n} \\sum E_{Y^{0}}\\left[L(Y_{i}^{0}, \\hat{m}(x_{i}) \\mid D \\right]\n\\]\nOptimism can be defined as\n\\[\n\\mbox{opp} \\equiv Err_{\\mbox{in}} - \\overline{err}\n\\]\nIt turns out there is a natural characterization of the expected optimism, or \\[\n\\omega = E_{\\mathbf{y}}[\\mbox{opp}]\n\\] Where the expectation is taken with respect to the training points \\(\\mathbf{y} = (y_i)_{i=1}^{n}\\)\n\n\\(\\omega = \\frac{2}{n}\\sum_{i}\\mbox{Cov}(y_{i}, \\hat{y}_i)\\)\n\n\nThe interpretation, to quote ESL\n\nThus the amount by which \\(\\overline{err}\\) underestimates the true error depends on how strongly \\(y_{i}\\) affects its own prediction. The harder we fit the data, the greater \\(Cov(\\hat{y}_i, y_i)\\) will be, thereby increasing the optimism.\n\nThe proof of this result is remarkably straightforward.\nFor ease of notation, let the new test data generated from the same points for the explanatory variables as the training set be denoted simply as \\(y_{i}^{\\prime} = Y_{i}^{0}\\)\nThe prediction error is then \\[\n\\frac{1}{n}\\sum\\left(y_{i}^{\\prime}-\\hat{y}_{i}\\right)^{2}\n\\]\nObserve that the \\(\\hat{y}_{i}'s\\) are functions of all the \\(y_{i}'s\\) and so are dependent random variables.\nHowever \\(y_{i}^{\\prime}\\) is independent of \\(\\hat{y}_{i}\\).\nRecall the formula\n\\[V\\left(Z\\right)=E\\left[Z^{2}\\right]-E\\left[Z\\right]^{2}\\implies E\\left[Z^{2}\\right]=V\\left[Z\\right]+E\\left[Z\\right]^{2}\\]\nThen \\[\\begin{align*}\nE\\left[\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\right] & =V\\left[y_{i}-\\hat{y}_{i}\\right]+E\\left[y_{i}-\\hat{y}_{i}\\right]^{2}\\\\\n & =V\\left[y_{i}\\right]+V\\left[\\hat{y}_{i}\\right]-2Cov\\left[y_{i},\\hat{y}_{i}\\right]+\\left(E\\left[y_{i}\\right]-E\\left[\\hat{y}_{i}\\right]\\right)^{2}\n\\end{align*}\\]\nObserve on the other hand that using the fact \\(E\\left[y_{i}\\right]=E\\left[y_{i}^{\\prime}\\right]\\) and \\(V\\left[y_{i}^{\\prime}\\right]=V\\left[y_{i}\\right]\\)\n\\[\\begin{align*}\nE\\left[\\left(y_{i}^{\\prime}-\\hat{y}_{i}\\right)^{2}\\right] & =V\\left[y_{i}^{\\prime}\\right]+V\\left[\\hat{y}_{i}\\right]-2Cov\\left[y_{i}^{\\prime},\\hat{y}_{i}\\right]+\\left(E\\left[y_{i}^{\\prime}\\right]-E\\left[\\hat{y}_{i}\\right]\\right)^{2}\\\\\n & =V\\left[y_{i}\\right]+V\\left[\\hat{y}_{i}\\right]+\\left(E\\left[y_{i}\\right]-E\\left[\\hat{y}_{i}\\right]\\right)^{2}\n\\end{align*}\\]\nHence we have the relationship that \\[\nE\\left[\\left(y_{i}^{\\prime}-\\hat{y}_{i}\\right)^{2}\\right]=E\\left[\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\right]+2Cov\\left[y_{i},\\hat{y}_{i}\\right].\n\\]\nAveraging over data points \\[\nE\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}^{\\prime}-\\hat{y}_{i}\\right)^{2}\\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\right]+\\frac{2}{n}\\sum_{i}Cov\\left(y_{i},\\hat{y}_{i}\\right).\n\\]\nWe can frame the result in a slightly different way for the specialized case where the true DGP is\n\\[\ny_{i} = m(x_i)+\\epsilon_i\n\\] For a homoskedastic \\(\\epsilon_i\\) with variance \\(\\sigma^2\\).\nLet us define the degrees of freedom \\(df\\left(\\hat{y}\\right)\\) of the fitted model \\(\\hat{y}\\) as \\[\ndf\\left(\\hat{y}\\right)=\\frac{1}{\\sigma^{2}}\\sum Cov\\left(y_{i},\\hat{y}_{i}\\right),\n\\]\nThen the relationship becomes \\[\nE\\left[\\left(y_{i}^{\\prime}-\\hat{y}_{i}\\right)^{2}\\right]=E\\left[\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\right]+\\frac{2\\sigma^{2}}{n}df\\left(\\hat{y}\\right)\n\\] which is words says that the expected test error is exactly the expected training error plus a constant factor \\(\\left(\\frac{2\\sigma^{2}}{n}\\right)\\) times the degree of freedom.\nThis gives us an approach to model selection. Suppose we have a family of fitted values \\(\\hat{y}_{\\eta}\\) which depends on a tuning parameter.\nThen we can estimate \\[\nErr_{\\mbox{in}}(\\eta) =\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{y}_{\\eta}\\right)_{i}\\right)^{2}+\\frac{2\\sigma^{2}}{n}df\\left(\\hat{y}_{\\eta}\\right)\n\\] replacing \\(\\sigma^{2}\\) and \\(df\\) with estimates if needed.\nThen we can select \\[\n\\hat{\\eta}=\\arg\\min_{\\eta\\in\\Theta}\\frac{1}{n}\\left(y_{i}-\\left(\\hat{y}_{\\eta}\\right)_{i}\\right)^{2}+\\frac{2\\sigma^{2}}{n}df\\left(\\hat{y}_{\\eta}\\right)\n\\]\nObserve that for a linear regression model this becomes \\[\\begin{align*}\ndf\\left(\\hat{y}^{linreg}\\right) & =\\\\\n\\frac{1}{\\sigma^{2}}Tr\\left(Cov\\left(X\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}Y\\right),Y\\right) & =\\\\\n\\frac{1}{\\sigma^{2}}Tr\\left(X\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}V\\left[Y\\right]\\right) & =\\\\\nTr\\left(X\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}\\right) & =\\\\\nTr\\left(X^{\\prime}X\\left(X^{\\prime}X\\right)^{-1}\\right) & =p\n\\end{align*}\\]\nThe optimism term \\(\\frac{2}{n}\\sigma^{2}\\left(p\\right)\\) depends on 3 key factors\nGrows with \\(\\sigma^{2}\\): more noise gives the model more opportunities to seem to fit well by capitalizing on chance.\nShrinks with \\(n\\): more data at any noise level makes it harder to pretend the fit is better than it is in reality.\nGrows with \\(p\\): every extra parameter is another control which can adjust to fit the noise.\n\nHence model selection on the number of predictors becomes \\[\n\\hat{k}=\\arg\\min_{k\\in\\left\\{ 1,\\dots,p\\right\\} }\\frac{1}{n}\\sum\\left(y_{i}-\\left(\\hat{y}_{k}\\right)_{i}\\right)^{2}+\\frac{2\\sigma^{2}}{n}k\n\\] which gives Mallow’s \\(C_{p}\\) criterion for choice among linear models.\nCross-Validation\nAn alternative approach to estimating the expected test error \\(Err\\) associated with an ML technique \\(\\hat{m}\\) is cross validation.\nLet the fitted value under a given ML technique class be \\(\\hat{y}_{\\eta}\\) for model tuning parameter \\(\\eta\\in\\Theta\\)\nGenerally \\(\\eta\\) is ordered such that larger or smaller values are associated with a higher degree of regularization in the method. We wish to select the value \\(\\eta^{\\ast}\\) that minimizes \\(Err\\).\nCross validation is a resampling technique that allows us to statistically approximate \\(Err\\)\nSplit the training set randomly into \\(K\\) divisions or folds for some number \\(K\\). Express the folds as \\(F_{1},\\dots,F_{K}\\) where \\[F_{1}\\cup \\dots \\cup F_{K}= \\{ 1,\\dots,n\\} \\] with \\(n_{k}=\\left|F_{k}\\right|\\) points in fold \\(k\\).\nFor each \\(k=1,\\dots,K\\), we fit our model to all points besides the \\(k^{th}\\) fold, and let the \\(i^{th}\\) fitted values be denotes \\(\\hat{m}_{\\eta}^{-k}\\left(x_{i}\\right)\\).\nWe then evaluate the error on the points in the \\(k^{th}\\) fold \\[\nCV_{k}\\left(\\eta\\right)=\\frac{1}{n_{k}}\\sum_{i\\in F_{k}}\\left(y_{i}-\\hat{m}_{\\eta}^{-k}\\left(x_{i}\\right)\\right)^{2}\n\\]\nWe then average over the folds to estimate prediction error \\[\nCV\\left(\\eta\\right)=\\frac{1}{K}CV_{k}\\left(\\eta\\right)\n\\]\nThe process can be depicted as\n\n\n\nThis is \\(K\\)-fold cross validation, and the tuning procedure becomes \\[\n\\hat{\\eta}=\\arg\\min_{\\eta\\in\\left\\{ \\eta_{1},\\dots,\\eta_{m}\\right\\} }CV\\left(\\eta\\right)\n\\]\nThis leaves open the choice of \\(K\\).\nFor \\(K=2\\) we have split sample cross-validation. The problem is that the CV error estimate will be biased upwards.\nFor \\(K=n\\) we have leave-one-out cross validation. The problem is that the CV error estimate will have high variance.\nA standard to balance this bias-variance tradeoff is setting \\(K=5\\) or \\(K=10\\), where each iteration we train on a fraction of about \\(\\frac{K-1}{K}\\) of the total training set so we reduce bias, and there is less overlap among training set, thus reducing bias.\nRecognizing there is some variance in the CV error estimate, the one standard error rule is an alternative to choice of \\(\\eta\\). Let \\[\nSD\\left(\\eta\\right)=\\sqrt{var\\left(CV_{1}\\left(\\eta\\right),\\dots,CV_{K}\\left(\\eta\\right)\\right)}\n\\] and \\[\nSE\\left(\\eta\\right)=\\frac{SD\\left(\\eta\\right)}{\\sqrt{K}}\n\\] is standard error estimate of \\(CV\\left(\\eta\\right).\\)\nThe one standard error rule is to move \\(\\eta\\) in the direction of increasing regularization until it ceases to be true that \\[\nCV\\left(\\eta\\right)\\leq CV\\left(\\hat{\\eta}\\right)+SE\\left(\\hat{\\eta}\\right)\n\\] e.g., we take the most simplest model whose error is within one standard error of the minimal error.\nSee the interesting discussion on the proper application of CV at the top of page 245.\nLet us now revisit the Ames, Iowa data to see if multidimensional tuning would play a role.\n\n\nlibrary(rsample)     # data splitting \nlibrary(dplyr)       # data wrangling\nlibrary(rpart)       # performing regression trees\nlibrary(rpart.plot)  # plotting regression trees\n\n\n\nCreate training and test samples.\n\n\n# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.\n# Use set.seed for reproducibility\n\nset.seed(123)\names_split <- initial_split(AmesHousing::make_ames(), prop = .7)\names_train <- training(ames_split)\names_test  <- testing(ames_split)\n\n\n\nSetup a grid for the tuning parameters\n\n\nhyper_grid <- expand.grid(\n  minsplit = seq(5, 20, 1),\n  maxdepth = seq(8, 15, 1)\n)\n\nhead(hyper_grid)\n\n\n  minsplit maxdepth\n1        5        8\n2        6        8\n3        7        8\n4        8        8\n5        9        8\n6       10        8\n\nUse CART for each tuning parameter value in the grid.\n\n\nmodels <- list()\n\nfor (i in 1:nrow(hyper_grid)) {\n  \n  # get minsplit, maxdepth values at row i\n  minsplit <- hyper_grid$minsplit[i]\n  maxdepth <- hyper_grid$maxdepth[i]\n\n  # train a model and store in the list\n  models[[i]] <- rpart(\n    formula = Sale_Price ~ .,\n    data    = ames_train,\n    method  = \"anova\",\n    control = list(minsplit = minsplit, maxdepth = maxdepth)\n    )\n}\n\n\n\n\n\n# function to get optimal cp\nget_cp <- function(x) {\n  min    <- which.min(x$cptable[, \"xerror\"])\n  cp <- x$cptable[min, \"CP\"] \n}\n\n# function to get minimum error\nget_min_error <- function(x) {\n  min    <- which.min(x$cptable[, \"xerror\"])\n  xerror <- x$cptable[min, \"xerror\"] \n}\n\nhyper_grid %>%\n  mutate(\n    cp    = purrr::map_dbl(models, get_cp),\n    error = purrr::map_dbl(models, get_min_error)\n    ) %>%\n  arrange(error) %>%\n  top_n(-5, wt = error)\n\n\n  minsplit maxdepth         cp     error\n1       19       12 0.01060346 0.2628987\n2        5        8 0.01000000 0.2635207\n3        9       11 0.01000000 0.2645615\n4       14       11 0.01000000 0.2650862\n5       13       10 0.01000000 0.2655860\n\nExtract the tree for the optimal value of the tuning parameters.\n\n\noptimal_tree <- rpart(\n    formula = Sale_Price ~ .,\n    data    = ames_train,\n    method  = \"anova\",\n    control = list(minsplit = 17, maxdepth = 12, cp = 0.01)\n    )\n\n\n\nThe new test error\n\n\npred <- predict(optimal_tree, newdata = ames_test)\np_error <- Metrics::rmse(actual = ames_test$Sale_Price, predicted = pred)\np_error\n\n\n[1] 39852.01\n\n\n\n\n",
    "preview": "https://i0.wp.com/blog.rankone.io/wp-content/uploads/2018/11/accuracy1.png?fit=1801%2C901&ssl=1",
    "last_modified": "2021-02-04T06:48:04+00:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "A First Look at Algorithmic Modeling : Regression Trees",
    "description": "An example of a regression tree in action",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\n\nContents\n\n\nAlgorithmic models begin with an ``expressive space’’ for representing the non-parametric primitive of interest (see previous post for the 3 main classes of non-parametric primitives). In the case of supervised learning, we seek a space to represent the conditional mean of \\(Y\\) given \\(X\\), which is the theoretically optimal predictor of \\(Y\\) from \\(X\\) under squared error loss\nThe algorithms themselves are concerned with finding a particular representation in the expressive space to fit the data at hand. The behavior of the algorithm for conducting this search is governed by a set of tuning parameters \\(\\eta\\). The process of finding the optimal tuning parameters is called (rather simply) tuning,\nDecision Trees\nThe general idea of prediction/decision trees is that we will partition the predictor space into a number of simple, rectangular regions.\nIn order to make a prediction for a given observation, we typically use the mean of the training data in the region to which it belongs.\nRectangles can be achieved by making successive binary splits on the predictors variables \\(X_1,\\ldots X_p\\). i.e., we choose a variable \\(X_j\\), \\(j=1,\\ldots p\\), and  divide  up the feature space according to \\[X_j \\leq c \\;\\;\\;\\text{and}\\;\\;\\; X_j > c\\] Then we proceed on each half\nThe splitting rules can be summarized visually as a tree, these approaches are called decision tree methods.\n\nConstructing a partitioned space from a decision tree\n\n\n\n\nThese methods are both simple and useful for interpretation.\nNote however that not every partition of the predictor space can be represented as a tree\n\n\n\nHow to use trees?\nFor an observation falling in region \\(R_{j}\\), we predict a value for the the outcome \\(Y\\) as \\(c_{j}\\)\nThe estimated regression function has the form \\[\\hat{f}^\\mathrm{tree}(x) \\;=\\; \\sum_{j=1}^m c_j \\cdot 1\\{x \\in R_j\\}\n\\;=\\; c_j \\;\\, \\mathrm{such}\\;\\mathrm{that}\\;\\, x \\in R_j\\]\nIn the case of a regression problem, the \\(c_j\\) are real numbers. How would we choose these? Simple: just take the average response of all of the points in the region. Note that each region \\(R_j\\) contains some subset of the training data \\((x_i,y_i)\\), \\(i=1,\\ldots n\\), say, \\(n_j\\) points. Thus \\[c_j = \\frac{1}{n_j} \\sum_{x_i \\in R_j} y_i\\]\nIn the case of a classification problem, the prediction is a class \\(c_{j} \\in \\{1,\\dots,K\\}\\) The predicted class \\(c_j\\) is just most common occuring class among these points.\nIn particular, for each class \\(k=1,\\ldots K\\), we can estimate the probability that the class label is \\(k\\) given that the feature vector lies in region \\(R_j\\), \\(\\mathrm{P}(C=k|X\\in R_j)\\), by \\[\\hat{p}_k (R_j) = \\frac{1}{n_j}\\sum_{x_i \\in R_j} 1\\{y_i = k\\}\\] the  proportion of points  in the region that are of class \\(k\\).\nWe can now express the predicted class as \\[c_j = \\mathop{\\mathrm{argmax}}_{k=1,\\ldots K} \\; \\hat{p}_k (R_j)\\]\nPrediction of baseball player’s salary\nSuppose we wish predict a baseball player’s Salary based on Years (the number of years that he has played in the major leagues) and Hits (the number of hits that he made in the previous year).\nSalary is measured in thousands of dollars.\n Prediction of baseball player’s salary \nFigure 1\n\n\n\nThe regression tree depicts the prediction of the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year. At a given internal node, the label (of the form \\(X_j < t_k\\)) indicates the left-hand branch resulting from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k.\\)\nThe tree represents a series of splits starting at the top of the tree.\nThe top split assigns observations having \\(Years < 4.5\\) to the left branch.\nThe predicted salary for these players is given by the mean response value for the players in the data set with \\(Years < 4.5.\\)\nFor such players, the mean log salary is 5.107, and so we make a prediction of \\(e^{5.107}\\) thousands of dollars, i.e. \\(165,174\\)\nAnother way to see the result of the tree is through the rectangular partition of the predictor space.\nFigure 2\n\n\n\nWe can write these regions as the following:\n\\(R_1 ={X \\mid Years < 4.5}\\)\n\\(R_2 ={X \\mid Years \\geq 4.5, Hits < 117.5}\\)\n\\(R_3 = {X \\mid Years \\geq 4.5, Hits \\geq 117.5}.\\)\n\nNotice the powerful appeal of interpretability of the resulting tree - even without a deep domain understanding of baseball, there is an intuitive sense that emerges for how baseball salaries work.\nTerminology\nIn keeping with the tree analogy, the regions \\(R_1\\), \\(R_2\\), …, \\(R_{J}\\) are known as terminal nodes or leaves of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes.\nIn Figure 1, the two internal nodes are indicated by the text \\(Years<4.5\\) and \\(Hits<117.5.\\)\nWe refer to the segments of the trees that connect the nodes as branches\nHow to build trees?\nThere are two main issues to consider in building a tree:\nHow to choose the splits?\nHow big to grow the tree?\n\nThink first about varying the depth of the tree …\nWhich is more complex - a big tree or a small tree?\nWhat  tradeoff  is at play here?\nHow might we eventually consider choosing the depth?\n\nNow for a fixed depth, consider choosing the splits. -If the tree has depth \\(d\\), then it has \\(\\approx 2^d\\) nodes.\nAt each node we could choose any of \\(p\\) the variables for the split\nThis means that the number of possibilities is \\[p \\cdot 2^d\\]\nThis is  huge  even for moderate \\(d\\)! And we haven’t even counted the actual split points themselves\n\nThe CART Algorithm\nThe  CART algorithm 1 chooses the splits in a top down fashion: then chooses the first variable to at the root, then the variables at the second level, etc.\nAt each node \\(t\\) encountered along the algorithm (starting with the root node), we have a measure of node impurity \\(Q_{t} = Q(D_{t})\\), which is a function of the data set \\(D_{t}\\) arriving at node \\(t\\). Let \\(c_{t}\\) be the predicted value for node \\(t\\).\nFor regression trees, a common impurity measure is the variance \\[\\frac{1}{N_{t}} \\sum_{i=1}^{N_{t}} (y_{i,t} - \\bar{y}_t)^{2}\\]\nFor classification trees, a common impurity measure is the mis-classification rate \\[1 - \\hat{p}_{c_{t}}\\] where \\(c_{t}\\) is the predicted class for node \\(t\\).\nAlternatives to misclassification error for classification trees are the Gini index or cross-entropy.\n\nAt node \\(t\\) we must determine the optimal split \\((j,s)\\) where \\(j\\) indexes the predictor variable \\(X_{j}\\) we split and \\(s\\) the split point. Let \\(D_{left}\\) and \\(D_{right}\\) be the induced data sets going to the left and right nodes from the split \\((j,s)\\), with number of observations \\(N_{left}\\) and \\(N_{right}\\) respectively.\nAt each step of the CART algorithm we seek to maximize the information gain \\(IG\\) of the split as measured by \\[IG(D,j, s) = Q(D) - \\frac{N_{left}}{N} Q(D_{left}) - \\frac{N_{right}}{N} Q(D_{right})\\] where \\(D\\) is the dataset of the parent node.\nThe CART algorithm greedily chooses the split to minimize \\[N_{left} Q(D_{left}) + N_{right} Q(D_{right})\\]\nHaving done this, we now recursively repeat the process for each new node arising from the split, treating \\(D_{left}\\) and \\(D_{right}\\) as parent data sets.\nHow do we find the best split \\(s\\)? Aren’t there  infinitely many  to consider? No, to split at node \\(t\\) on a variable \\(j\\), we really only need to consider \\(N_t\\) splits (or \\(N_t-1\\) splits)\n\n\n\nWhen does the process stop? stopping We stop only when each terminal node has fewer than some minimum number of observations (e.g., 10 observations).\nPruning the tree\nContinuing on in this manner, we will get a big tree \\(T_0\\). Let the size \\(\\tilde{T}_{0} = |T_{0}|\\) be denoted by the number of leaf nodes.\nA natural concern with CART is overfitting - we have grown the tree to large.\nWe then  prune  this tree, meaning that we collapse some of its leaves into the parent nodes.\nNotice that the total information gain from building the tree is \\[Q(D) - \\sum_{t}^{T} \\frac{N_{t}}{N}Q(D_t)\\] where \\(D\\) is the data entering the root node (e.g., the full training sample)\nWe seek to find the subtree \\(T \\subset T_{0}\\) to minimize the cost complexity criterion \\[C_{\\alpha}(T) = \\sum_{t=1}^{|T|} N_{t}Q_t + \\alpha |T|\\]\nWhat is the logic of this approach? That is, why grow a large tree and prune, instead of just stopping at some point when the information gain ? Because any stopping rule may be  short-sighted , in that a split may look bad but it may lead to a good split below it.\n\n\n\nFor any given \\(\\alpha\\), the optimal pruned tree \\(T(\\alpha)\\) can be found via a convenient algorithm - weakest link pruning.\nWhen \\(\\alpha = 0,\\) then the subtree T will simply equal \\(T_0\\), because then the penalized objective just measures the training error.\nHowever, as \\(\\alpha = 0\\) increases, there is a price to pay for having a tree with many terminal nodes, and so the penalized objective will be minimized for a smaller sub-tree.\nWe start from the full tree \\(T_{0}\\)\nFor any internal node \\(t\\),we let \\(T{t}\\) be the branch of \\(T\\) with root \\(t\\) (e.g., the pruned part if we snip the tree at node \\(t\\))\nThe cost complexity criterion would become smaller by pruning at \\(t\\) if the following held\n\\[ N_{t} Q(D_{t}) + \\alpha  < \\sum _{t^\\prime = 1}^{|T_{t}|} N_{t^\\prime}Q(D_{t^\\prime}) + \\alpha|T_{t}| \\]\nIt pays to prune at \\(t\\) if \\[\\alpha > \\frac{N_{t} Q(D_{t}) - \\sum _{t^\\prime = 1}^{|T_{t}|} N_{t^\\prime}Q(D_{t^\\prime})}{|T_{t}| - 1} = g_{0}(t)\\]\nObserve that \\(\\alpha\\) is always positive because the numerator of the RHS is a re-scaled version of the information gain from splitting at \\(t\\). this can be done by pruning the weakest leaf one at a time.\nDefine the weakest link in \\(T_{0}\\) as the internal node \\(t_{0}\\) such that \\(g_{0}(t_{0}) = \\min g_{0}(t)\\), and let \\(\\alpha_{1} = g_{0}(t_{0})\\)\nPrune the tree \\(T_{0} - T_{1}\\) and repeat the weakest link process.\nThis gives us a decreasing sequence of trees \\[T_{0} \\prec T_{1} \\dots \\prec t_{root}\\] all the way down to the root node \\(t_{root}\\), as well as an increasing sequence of \\(\\alpha\\) values \\[0 < \\alpha_{1} < \\alpha_{2} < \\dots < \\alpha_{K}\\]\nA key result in Breiman et al (1984) is that for \\(\\alpha\\) such that \\(\\alpha_{k} < \\alpha < \\alpha_{k+1}\\), the optimum tree \\(T(\\alpha)\\) is \\(T_{k}\\).\n\\(\\alpha\\) is a  tuning parameter , and a larger \\(\\alpha\\) yields a smaller tree.\nThus as \\(\\alpha = 0\\) increases from 0, branches are pruned from the tree in a nested and predictable way (resulting in the whole sequence of subtrees as a function of \\(\\alpha\\)).\nCART picks from the finite menu of potential \\(\\alpha_{k}\\) by 5- or 10-fold cross-validation\nAlgorithm for Building a Regression Tree\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha.\\)\nUse \\(K\\)-fold cross-validation to choose \\(\\alpha.\\) That is, divide the training observations into \\(K\\) folds. For each \\(k = 1, \\ldots, K\\):\nRepeat Steps 1 and 2 on all but the kth fold of the training data.\nEvaluate the mean squared prediction error on the data in the left-out kth fold, as a function of \\(\\alpha.\\)\nAverage the results for each value of \\(\\alpha\\), and pick \\(\\alpha\\) to minimize the average error.\n\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha.\\)\nSome considerations\nSplitting a categorical variable predictor variable with many levels can be onerous, e.g., a categorical variable with \\(q\\) levels has \\(2^{q-1} - 1\\) potential splits, and thus need to carefully consider category definitions in using such predictors.\nAn Empirical Example: Spam Data\n-In an empirical analysis, we will split our data \\(D\\) into a test \\(D_{test}\\) and training \\(D_{train}\\) set.\nWe will run the CART algorithm (grow and prune a tree) on \\(D_{train}\\) to extract a prediction tree, and perform predictions on the \\(D_{test}\\)\nCross validation is usually done with 5 or 10 folds, and the “one standard error” rule is applied (the simplest model with performance at the min of the CV performance + one standard error)\nExample from ESL: \\(n=4601\\) emails, of which 1813 are considered spam. For each email we have \\(p=58\\) attributes. The first 54 measure the frequencies of 54 key words or characters (e.g., “free”, “need”, “$”). The last 3 measure\nthe average length of uninterrupted sequences of capitals;\nthe length of the longest uninterrupted sequence of capitals;\nthe sum of lengths of uninterrupted sequences of capitals\n\nCross-validation error curve for the spam data (from ESL page 314):\n\n\n\nThe blue curve is the 10-fold cross validation estimate of the mis-classification rate with SE bars. The orange curve is the test error, which tracks the CV error closely.\nThe min occurs at a tree of size 17 (with one SE rule applied)\n\n\n\nRunning CART in R\nThe package rpart provides a fast implementation and interface for the CART algorithm and decision trees in R. We consider an example here with the Ames, Iowa Housing Data (sometimes called the new iris data). For a description, see here\nWe can first load the data\n\n\nlibrary(rsample)     # data splitting \nlibrary(dplyr)       # data wrangling\nlibrary(rpart)       # performing regression trees\nlibrary(rpart.plot)  # plotting regression trees\nlibrary(skimr)\n\n\n\nLets skim the data.\n\nTable 1: Data summary\nName\nAmesHousing::make_ames()\nNumber of rows\n2930\nNumber of columns\n81\n_______________________\n\nColumn type frequency:\n\nfactor\n46\nnumeric\n35\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\nOverall_Qual\n0\n1\nFALSE\n10\nAve: 825, Abo: 732, Goo: 602, Ver: 350\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\nExter_Qual\n0\n1\nFALSE\n4\nTyp: 1799, Goo: 989, Exc: 107, Fai: 35\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\nBsmt_Qual\n0\n1\nFALSE\n6\nTyp: 1283, Goo: 1219, Exc: 258, Fai: 88\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\nKitchen_Qual\n0\n1\nFALSE\n5\nTyp: 1494, Goo: 1160, Exc: 205, Fai: 70\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\nFireplace_Qu\n0\n1\nFALSE\n6\nNo_: 1422, Goo: 744, Typ: 600, Fai: 75\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\nGarage_Qual\n0\n1\nFALSE\n6\nTyp: 2615, No_: 159, Fai: 124, Goo: 24\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\n▇▇▁▁▁\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\n▇▁▁▁▁\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\n▁▂▃▆▇\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\n▅▂▂▃▇\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\n▇▁▁▁▁\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\n▃▂▇▁▇\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\n▇▁▁▁▁\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\n▇▅▂▁▁\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\n▇▃▁▁▁\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\n▇▃▁▁▁\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\n▇▃▂▁▁\nLow_Qual_Fin_SF\n0\n1\n4.68\n46.31\n0.00\n0.00\n0.00\n0.00\n1064.00\n▇▁▁▁▁\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\n▇▇▁▁▁\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\n▇▆▁▁▁\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\n▁▇▇▁▁\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▁\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\n▁▇▂▁▁\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\n▁▇▁▁▁\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\n▁▇▂▁▁\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\n▇▇▁▁▁\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\n▅▇▂▁▁\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\n▃▇▃▁▁\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\n▇▁▁▁▁\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\n▇▁▁▁▁\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\n▇▁▁▁▁\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\n▇▁▁▁▁\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\n▇▁▁▁▁\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\n▇▁▁▁▁\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\n▇▁▁▁▁\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\n▅▆▇▃▃\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\n▇▇▇▇▃\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\n▇▇▁▁▁\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\n▅▅▇▆▁\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\n▂▂▇▇▇\n\nCreate training and test samples.\n\n\n# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.\n# Use set.seed for reproducibility\n\nset.seed(123)\names_split <- initial_split(AmesHousing::make_ames(), prop = .7)\names_train <- training(ames_split)\names_test  <- testing(ames_split)\n\n\n\nConstruct a tree on the training sample\n\n\nm1 <- rpart(\n  formula = Sale_Price ~ .,\n  data    = ames_train,\n  method  = \"anova\"\n  )\n\nclass(m1)\n\n\n[1] \"rpart\"\n\nm1\n\n\nn= 2051 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 2051 1.273987e+13 180775.50  \n   2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1703 4.032269e+12 156431.40  \n     4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste 1015 1.360332e+12 131803.50  \n       8) First_Flr_SF< 1048.5 611 4.924281e+11 118301.50  \n        16) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 152 1.053743e+11  91652.57 *\n        17) Overall_Qual=Average,Above_Average,Good 459 2.433622e+11 127126.40 *\n       9) First_Flr_SF>=1048.5 404 5.880574e+11 152223.50  \n        18) Gr_Liv_Area< 2007.5 359 2.957141e+11 145749.50 *\n        19) Gr_Liv_Area>=2007.5 45 1.572566e+11 203871.90 *\n     5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 688 1.148069e+12 192764.70  \n      10) Gr_Liv_Area< 1725.5 482 5.162415e+11 178531.00  \n        20) Total_Bsmt_SF< 1331 352 2.315412e+11 167759.00 *\n        21) Total_Bsmt_SF>=1331 130 1.332603e+11 207698.30 *\n      11) Gr_Liv_Area>=1725.5 206 3.056877e+11 226068.80 *\n   3) Overall_Qual=Very_Good,Excellent,Very_Excellent 348 2.759339e+12 299907.90  \n     6) Overall_Qual=Very_Good 249 9.159879e+11 268089.10  \n      12) Gr_Liv_Area< 1592.5 78 1.339905e+11 220448.90 *\n      13) Gr_Liv_Area>=1592.5 171 5.242201e+11 289819.70 *\n     7) Overall_Qual=Excellent,Very_Excellent 99 9.571896e+11 379937.20  \n      14) Gr_Liv_Area< 1947 42 7.265064e+10 325865.10 *\n      15) Gr_Liv_Area>=1947 57 6.712559e+11 419779.80  \n        30) Neighborhood=Old_Town,Edwards,Timberland 7 8.073100e+10 295300.00 *\n        31) Neighborhood=College_Creek,Somerset,Northridge_Heights,Northridge,Stone_Brook 50 4.668730e+11 437207.00  \n          62) Total_Bsmt_SF< 2168.5 40 1.923959e+11 408996.90 *\n          63) Total_Bsmt_SF>=2168.5 10 1.153154e+11 550047.30 *\n\nPlot the fitted tree\n\n\n\nVisualize the cross validation experiment\n\n\nplotcp(m1)\n\n\n\n\nWe can force a larger tree to be fit\n\n\nm2 <- rpart(\n    formula = Sale_Price ~ .,\n    data    = ames_train,\n    method  = \"anova\", \n    control = list(cp = 0, xval = 10)\n)\n\nplotcp(m2)\n\n\n\n\nThe data underlying cross validation can be extracted\n\n\nm1$cptable\n\n\n           CP nsplit rel error    xerror       xstd\n1  0.46690132      0 1.0000000 1.0009222 0.05855161\n2  0.11961409      1 0.5330987 0.5347929 0.03116217\n3  0.06955813      2 0.4134846 0.4151417 0.03058554\n4  0.02559992      3 0.3439265 0.3461258 0.02207839\n5  0.02196620      4 0.3183265 0.3242197 0.02182111\n6  0.02023390      5 0.2963603 0.3074877 0.02129292\n7  0.01674138      6 0.2761264 0.2963372 0.02106996\n8  0.01188709      7 0.2593850 0.2795199 0.01903482\n9  0.01127889      8 0.2474980 0.2762666 0.01936472\n10 0.01109955      9 0.2362191 0.2699895 0.01902217\n11 0.01060346     11 0.2140200 0.2672133 0.01883219\n12 0.01000000     12 0.2034165 0.2635207 0.01881691\n\nNow predict on the test set:\n\n\npred <- predict(m1, newdata = ames_test)\np_error <- Metrics::rmse(actual = ames_test$Sale_Price, predicted = pred)\np_error\n\n\n[1] 39852.01\n\ne.g.,the average distance between predicted values and actuals is 39,852 dollars.\n\nBreiman et al. (1984), ``Classification and Regression Trees’’↩︎\n",
    "preview": "https://static.packt-cdn.com/products/9781788830577/graphics/a480732e-a17a-4220-8b7d-e04d7430bce1.png",
    "last_modified": "2021-02-04T06:45:59+00:00",
    "input_file": {}
  }
]
