[
  {
    "path": "posts/welcome/",
    "title": "A First Look at Algorithmic Modeling : Regression Trees",
    "description": "An example of a regression tree in action",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\n\nContents\n\n\nAlgorithmic models begin with an ``expressive space’’ for representing the non-parametric primitive of interest (see previous post for the 3 main classes of non-parametric primitives). In the case of supervised learning, we seek a space to represent the conditional mean of \\(Y\\) given \\(X\\), which is the theoretically optimal predictor of \\(Y\\) from \\(X\\) under squared error loss\nThe algorithms themselves are concerned with finding a particular representation in the expressive space to fit the data at hand. The behavior of the algorithm for conducting this search is governed by a set of tuning parameters \\(\\eta\\). The process of finding the optimal tuning parameters is called (rather simply) tuning,\nDecision Trees\nThe general idea of prediction/decision trees is that we will partition the predictor space into a number of simple, rectangular regions.\nIn order to make a prediction for a given observation, we typically use the mean of the training data in the region to which it belongs.\nRectangles can be achieved by making successive binary splits on the predictors variables \\(X_1,\\ldots X_p\\). i.e., we choose a variable \\(X_j\\), \\(j=1,\\ldots p\\), and  divide  up the feature space according to \\[X_j \\leq c \\;\\;\\;\\text{and}\\;\\;\\; X_j > c\\] Then we proceed on each half\nThe splitting rules can be summarized visually as a tree, these approaches are called decision tree methods.\n\nConstructing a partitioned space from a decision tree\n\n\n\n\nThese methods are both simple and useful for interpretation.\nNote however that not every partition of the predictor space can be represented as a tree\n\n\n\nHow to use trees?\nFor an observation falling in region \\(R_{j}\\), we predict a value for the the outcome \\(Y\\) as \\(c_{j}\\)\nThe estimated regression function has the form \\[\\hat{f}^\\mathrm{tree}(x) \\;=\\; \\sum_{j=1}^m c_j \\cdot 1\\{x \\in R_j\\}\n\\;=\\; c_j \\;\\, \\mathrm{such}\\;\\mathrm{that}\\;\\, x \\in R_j\\]\nIn the case of a regression problem, the \\(c_j\\) are real numbers. How would we choose these? Simple: just take the average response of all of the points in the region. Note that each region \\(R_j\\) contains some subset of the training data \\((x_i,y_i)\\), \\(i=1,\\ldots n\\), say, \\(n_j\\) points. Thus \\[c_j = \\frac{1}{n_j} \\sum_{x_i \\in R_j} y_i\\]\nIn the case of a classification problem, the prediction is a class \\(c_{j} \\in \\{1,\\dots,K\\}\\) The predicted class \\(c_j\\) is just most common occuring class among these points.\nIn particular, for each class \\(k=1,\\ldots K\\), we can estimate the probability that the class label is \\(k\\) given that the feature vector lies in region \\(R_j\\), \\(\\mathrm{P}(C=k|X\\in R_j)\\), by \\[\\hat{p}_k (R_j) = \\frac{1}{n_j}\\sum_{x_i \\in R_j} 1\\{y_i = k\\}\\] the  proportion of points  in the region that are of class \\(k\\).\nWe can now express the predicted class as \\[c_j = \\mathop{\\mathrm{argmax}}_{k=1,\\ldots K} \\; \\hat{p}_k (R_j)\\]\nPrediction of baseball player’s salary\nSuppose we wish predict a baseball player’s Salary based on Years (the number of years that he has played in the major leagues) and Hits (the number of hits that he made in the previous year).\nSalary is measured in thousands of dollars.\n Prediction of baseball player’s salary \nFigure 1\n\n\n\nThe regression tree depicts the prediction of the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year. At a given internal node, the label (of the form \\(X_j < t_k\\)) indicates the left-hand branch resulting from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k.\\)\nThe tree represents a series of splits starting at the top of the tree.\nThe top split assigns observations having \\(Years < 4.5\\) to the left branch.\nThe predicted salary for these players is given by the mean response value for the players in the data set with \\(Years < 4.5.\\)\nFor such players, the mean log salary is 5.107, and so we make a prediction of \\(e^{5.107}\\) thousands of dollars, i.e. \\(165,174\\)\nAnother way to see the result of the tree is through the rectangular partition of the predictor space.\nFigure 2\n\n\n\nWe can write these regions as the following:\n\\(R_1 ={X \\mid Years < 4.5}\\)\n\\(R_2 ={X \\mid Years \\geq 4.5, Hits < 117.5}\\)\n\\(R_3 = {X \\mid Years \\geq 4.5, Hits \\geq 117.5}.\\)\n\nNotice the powerful appeal of interpretability of the resulting tree - even without a deep domain understanding of baseball, there is an intuitive sense that emerges for how baseball salaries work.\nTerminology\nIn keeping with the tree analogy, the regions \\(R_1\\), \\(R_2\\), …, \\(R_{J}\\) are known as terminal nodes or leaves of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes.\nIn Figure 1, the two internal nodes are indicated by the text \\(Years<4.5\\) and \\(Hits<117.5.\\)\nWe refer to the segments of the trees that connect the nodes as branches\nHow to build trees?\nThere are two main issues to consider in building a tree:\nHow to choose the splits?\nHow big to grow the tree?\n\nThink first about varying the depth of the tree …\nWhich is more complex - a big tree or a small tree?\nWhat  tradeoff  is at play here?\nHow might we eventually consider choosing the depth?\n\nNow for a fixed depth, consider choosing the splits. -If the tree has depth \\(d\\), then it has \\(\\approx 2^d\\) nodes.\nAt each node we could choose any of \\(p\\) the variables for the split\nThis means that the number of possibilities is \\[p \\cdot 2^d\\]\nThis is  huge  even for moderate \\(d\\)! And we haven’t even counted the actual split points themselves\n\nThe CART Algorithm\nThe  CART algorithm 1 chooses the splits in a top down fashion: then chooses the first variable to at the root, then the variables at the second level, etc.\nAt each node \\(t\\) encountered along the algorithm (starting with the root node), we have a measure of node impurity \\(Q_{t} = Q(D_{t})\\), which is a function of the data set \\(D_{t}\\) arriving at node \\(t\\). Let \\(c_{t}\\) be the predicted value for node \\(t\\).\nFor regression trees, a common impurity measure is the variance \\[\\frac{1}{N_{t}} \\sum_{i=1}^{N_{t}} (y_{i,t} - \\bar{y}_t)^{2}\\]\nFor classification trees, a common impurity measure is the mis-classification rate \\[1 - \\hat{p}_{c_{t}}\\] where \\(c_{t}\\) is the predicted class for node \\(t\\).\nAlternatives to misclassification error for classification trees are the Gini index or cross-entropy.\n\nAt node \\(t\\) we must determine the optimal split \\((j,s)\\) where \\(j\\) indexes the predictor variable \\(X_{j}\\) we split and \\(s\\) the split point. Let \\(D_{left}\\) and \\(D_{right}\\) be the induced data sets going to the left and right nodes from the split \\((j,s)\\), with number of observations \\(N_{left}\\) and \\(N_{right}\\) respectively.\nAt each step of the CART algorithm we seek to maximize the information gain \\(IG\\) of the split as measured by \\[IG(D,j, s) = Q(D) - \\frac{N_{left}}{N} Q(D_{left}) - \\frac{N_{left}}{N} Q(D_{right})\\] where \\(D\\) is the dataset of the parent node.\nThe CART algorithm greedily chooses the split to minimize \\[N_{left} Q(D_{left}) + N_{right} Q(D_{right})\\]\nHaving done this, we now recursively repeat the process for each new node arising from the split, treating \\(D_{left}\\) and \\(D_{right}\\) as parent data sets.\nHow do we find the best split \\(s\\)? Aren’t there  infinitely many  to consider? No, to split at node \\(t\\) on a variable \\(j\\), we really only need to consider \\(N_t\\) splits (or \\(N_t-1\\) splits)\n\nWhen does the process stop? stopping We stop only when each terminal node has fewer than some minimum number of observations (e.g., 10 observations).\nPruning the tree\nContinuing on in this manner, we will get a big tree \\(T_0\\). Let the size \\(\\tilde{T}_{0} = |T_{0}|\\) be denoted by the number of leaf nodes.\nA natural concern with CART is overfitting - we have grown the tree to large.\nWe then  prune  this tree, meaning that we collapse some of its leaves into the parent nodes.\nNotice that the total information gain from building the tree is \\[Q(D) - \\sum_{t}^{T} \\frac{N_{t}}{N}Q(D_t)\\] where \\(D\\) is the data entering the root node (e.g., the full training sample)\nWe seek to find the subtree \\(T \\subset T_{0}\\) to minimize the cost complexity criterion \\[C_{\\alpha}(T) = \\sum_{t=1}^{|T|} N_{t}Q_t + \\alpha |T|\\]\nWhat is the logic of this approach? That is, why grow a large tree and prune, instead of just stopping at some point when the information gain ? Because any stopping rule may be  short-sighted , in that a split may look bad but it may lead to a good split below it.\n\n\n\nFor any given \\(\\alpha\\), the optimal pruned tree \\(T(\\alpha)\\) can be found via a convenient algorithm - weakest link pruning.\nWhen \\(\\alpha = 0,\\) then the subtree T will simply equal \\(T_0\\), because then the penalized objective just measures the training error.\nHowever, as \\(\\alpha = 0\\) increases, there is a price to pay for having a tree with many terminal nodes, and so the penalized objective will be minimized for a smaller sub-tree.\nWe start from the full tree \\(T_{0}\\)\nFor any internal node \\(t\\),we let \\(T{t}\\) be the branch of \\(T\\) with root \\(t\\) (e.g., the pruned part if we snip the tree at node \\(t\\))\nThe cost complexity criterion would become smaller by pruning at \\(t\\) if the following held\n\\[ N_{t} Q(D_{t}) + \\alpha  < \\sum _{t^\\prime = 1}^{|T_{t}|} N_{t^\\prime}Q(D_{t^\\prime}) + \\alpha|T_{t}| \\]\nIt pays to prune at \\(t\\) if \\[\\alpha > \\frac{N_{t} Q(D_{t}) - \\sum _{t^\\prime = 1}^{|T_{t}|} N_{t^\\prime}Q(D_{t^\\prime})}{|T_{t}| - 1} = g_{0}(t)\\]\nObserve that \\[\\alpha\\] is always positive because the numerator of the RHS is a re-scaled version of the information gain from splitting at \\(t\\). this can be done by pruning the weakest leaf one at a time.\nDefine the weakest link in \\(T_{0}\\) as the internal node \\(t_{0}\\) such that \\(g_{0}(t_{0}) = \\min g_{0}(t)\\), and let \\(\\alpha_{1} = g_{0}(t_{0})\\)\nPrune the tree \\(T_{0} - T_{1}\\) and repeat the weakest link process.\nThis gives us a decreasing sequence of trees \\[T_{0} \\prec T_{1} \\dots \\prec t_{root}\\] all the way down to the root node \\(t_{root}\\), as well as an increasing sequence of \\(\\alpha\\) values $0 < {1} < {2} < $\nA key result in Breiman et al (1984) is that for \\(\\alpha\\) such that \\(\\alpha_{k} < \\alpha < \\alpha_{k+1}\\), the optimum tree \\(T(\\alpha)\\) is \\(T_{k}\\).\n\\(\\alpha\\) is a  tuning parameter , and a larger \\(\\alpha\\) yields a smaller tree.\nThus as \\(\\alpha = 0\\) increases from 0, branches are pruned from the tree in a nested and predictable way (resulting in the whole sequence of subtrees as a function of \\(\\alpha\\)).\nCART picks from the finite menu of potential \\(\\alpha_{k}\\) by 5- or 10-fold cross-validation\nAlgorithm for Building a Regression Tree\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha.\\)\nUse \\(K\\)-fold cross-validation to choose \\(\\alpha.\\) That is, divide the training observations into \\(K\\) folds. For each \\(k = 1, \\ldots, K\\):\nRepeat Steps 1 and 2 on all but the kth fold of the training data.\nEvaluate the mean squared prediction error on the data in the left-out kth fold, as a function of \\(\\alpha.\\)\nAverage the results for each value of \\(\\alpha\\), and pick \\(\\alpha\\) to minimize the average error.\n\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha.\\)\nSome considerations\nSplitting a categorical variable predictor variable with many levels can be onerous, e.g., a categorical variable with \\(q\\) levels has \\(2^{q-1} - 1\\) potential splits, and thus need to carefully consider category definitions in using such predictors.\nAn Empirical Example: Spam Data\n-In an empirical analysis, we will split our data \\(D\\) into a test \\(D_{test}\\) and training \\(D_{train}\\) set.\nWe will run the CART algorithm (grow and prune a tree) on \\(D_{train}\\) to extract a prediction tree, and perform predictions on the \\(D_{test}\\)\nCross validation is usually done with 5 or 10 folds, and the “one standard error” rule is applied (the simplest model with performance at the min of the CV performance + one standard error)\nExample from ESL: \\(n=4601\\) emails, of which 1813 are considered spam. For each email we have \\(p=58\\) attributes. The first 54 measure the frequencies of 54 key words or characters (e.g., “free”, “need”, “$”). The last 3 measure\nthe average length of uninterrupted sequences of capitals;\nthe length of the longest uninterrupted sequence of capitals;\nthe sum of lengths of uninterrupted sequences of capitals\n\nCross-validation error curve for the spam data (from ESL page 314):\n\n\n\nThe blue curve is the 10-fold cross validation estimate of the mis-classification rate with SE bars. The orange curve is the test error, which tracks the CV error closely.\nThe min occurs at a tree of size 17 (with one SE rule applied)\n\n\n\nRunning CART in R\nThe package rpart provides a fast implementation and interface for the CART algorithm and decision trees in R. We consider an example here with the Ames, Iowa Housing Data (sometimes called the new iris data). For a description, see here\nWe can first load the data\n\n\nlibrary(rsample)     # data splitting \nlibrary(dplyr)       # data wrangling\nlibrary(rpart)       # performing regression trees\nlibrary(rpart.plot)  # plotting regression trees\nlibrary(skimr)\n\n\n\nLets skim the data.\n\nTable 1: Data summary\nName\nAmesHousing::make_ames()\nNumber of rows\n2930\nNumber of columns\n81\n_______________________\n\nColumn type frequency:\n\nfactor\n46\nnumeric\n35\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\nOverall_Qual\n0\n1\nFALSE\n10\nAve: 825, Abo: 732, Goo: 602, Ver: 350\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\nExter_Qual\n0\n1\nFALSE\n4\nTyp: 1799, Goo: 989, Exc: 107, Fai: 35\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\nBsmt_Qual\n0\n1\nFALSE\n6\nTyp: 1283, Goo: 1219, Exc: 258, Fai: 88\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\nKitchen_Qual\n0\n1\nFALSE\n5\nTyp: 1494, Goo: 1160, Exc: 205, Fai: 70\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\nFireplace_Qu\n0\n1\nFALSE\n6\nNo_: 1422, Goo: 744, Typ: 600, Fai: 75\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\nGarage_Qual\n0\n1\nFALSE\n6\nTyp: 2615, No_: 159, Fai: 124, Goo: 24\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\n▇▇▁▁▁\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\n▇▁▁▁▁\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\n▁▂▃▆▇\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\n▅▂▂▃▇\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\n▇▁▁▁▁\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\n▃▂▇▁▇\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\n▇▁▁▁▁\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\n▇▅▂▁▁\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\n▇▃▁▁▁\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\n▇▃▁▁▁\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\n▇▃▂▁▁\nLow_Qual_Fin_SF\n0\n1\n4.68\n46.31\n0.00\n0.00\n0.00\n0.00\n1064.00\n▇▁▁▁▁\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\n▇▇▁▁▁\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\n▇▆▁▁▁\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\n▁▇▇▁▁\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▁\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\n▁▇▂▁▁\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\n▁▇▁▁▁\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\n▁▇▂▁▁\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\n▇▇▁▁▁\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\n▅▇▂▁▁\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\n▃▇▃▁▁\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\n▇▁▁▁▁\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\n▇▁▁▁▁\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\n▇▁▁▁▁\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\n▇▁▁▁▁\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\n▇▁▁▁▁\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\n▇▁▁▁▁\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\n▇▁▁▁▁\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\n▅▆▇▃▃\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\n▇▇▇▇▃\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\n▇▇▁▁▁\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\n▅▅▇▆▁\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\n▂▂▇▇▇\n\nCreate training and test samples.\n\n\n# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.\n# Use set.seed for reproducibility\n\nset.seed(123)\names_split <- initial_split(AmesHousing::make_ames(), prop = .7)\names_train <- training(ames_split)\names_test  <- testing(ames_split)\n\n\n\nConstruct a tree on the training sample\n\n\nm1 <- rpart(\n  formula = Sale_Price ~ .,\n  data    = ames_train,\n  method  = \"anova\"\n  )\n\n\n\nPlot the fitted tree\n\n\n\nVisualize the cross validation experiment\n\n\nplotcp(m1)\n\n\n\n\nWe can force a larger tree to be fit\n\n\nm2 <- rpart(\n    formula = Sale_Price ~ .,\n    data    = ames_train,\n    method  = \"anova\", \n    control = list(cp = 0, xval = 10)\n)\n\nplotcp(m2)\n\n\n\n\nThe data underlying cross validation can be extracted\n\n\nm1$cptable\n\n\n           CP nsplit rel error    xerror       xstd\n1  0.46690132      0 1.0000000 1.0009222 0.05855161\n2  0.11961409      1 0.5330987 0.5347929 0.03116217\n3  0.06955813      2 0.4134846 0.4151417 0.03058554\n4  0.02559992      3 0.3439265 0.3461258 0.02207839\n5  0.02196620      4 0.3183265 0.3242197 0.02182111\n6  0.02023390      5 0.2963603 0.3074877 0.02129292\n7  0.01674138      6 0.2761264 0.2963372 0.02106996\n8  0.01188709      7 0.2593850 0.2795199 0.01903482\n9  0.01127889      8 0.2474980 0.2762666 0.01936472\n10 0.01109955      9 0.2362191 0.2699895 0.01902217\n11 0.01060346     11 0.2140200 0.2672133 0.01883219\n12 0.01000000     12 0.2034165 0.2635207 0.01881691\n\nNow predict on the test set:\n\n\npred <- predict(m1, newdata = ames_test)\np_error <- Metrics::rmse(actual = ames_test$Sale_Price, predicted = pred)\np_error\n\n\n[1] 39852.01\n\ne.g.,the average distance between predicted values and actuals is 39,852 dollars.\n\nBreiman et al. (1984), ``Classification and Regression Trees’’↩︎\n",
    "preview": "https://static.packt-cdn.com/products/9781788830577/graphics/a480732e-a17a-4220-8b7d-e04d7430bce1.png",
    "last_modified": "2021-02-02T14:49:34+00:00",
    "input_file": {}
  }
]
