---
title: "An Algorithmic Perspective on BLP-Style Demand Estimation"
description: |
  Predictive Performance 
author:
  - Jonathan Arnold 
  - Sheng Chao Ho
date: 03-02-2021
output:
  distill::distill_article:
    self_contained: false
categories:
  - Jonathan and Sheng  
---


<!-- Load some general packages, plus load and process data -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(width = 60)
ggplot2::theme_set(ggplot2::theme_minimal())
#setwd("./_posts/jonathan-sheng-algorithmic-demand-estimation-exploratory")
```

```{r load_packages, include=FALSE}
library(tidyverse)
library(ggplot2)
library(ggrepel)
library(skimr)
library(ranger)
library(vip)
library(tidymodels)
library(usemodels)
library(doParallel)
```

```{r load and process data, include=FALSE}

df_raw <- haven::read_dta("Model_panel_cleaned.dta", encoding = "ASCII")

#This chunk returns names of columns that do not vary within each year
zero_var_in_year <- 
  df_raw %>%  
  mutate(across(where(is.character), ~ factor(.x) %>% as.numeric)) %>% 
  group_by(Year) %>%
  summarise(across(.cols = everything(), ~ var(.x))) %>% 
  select(where(~ is.numeric(.x) && mean(.x,na.rm = TRUE) == 0.0)) %>% 
  colnames()

#Now get subset of df_raw that may be more relevant. 
df <- df_raw %>% 
  #remove columns that are duplicates (in the case of nationality1 and nationality2  they are "almost" duplicates of nationality)
  select(-starts_with("log"),
         -c(trim_name,
            transmission_id,drivetype_id,
            MY_id,Model_id,
            nationality1,nationality2,
            I_convertible,I_coupe,I_hatchback,I_pickup,I_sedan,I_suv,I_van,I_wagon)
         ) %>% 
  #remove the 2 observations without input for nationality
  filter(nationality!="") %>% 
  #the following part finds real price and market share of each product within a year 
  group_by(Year) %>% 
  mutate(share = sales*price / sum(sales*price),
         realprice = price*100/cpi) %>% 
  #the following part finds the average price of a firm's competitors' products in each year, and also the average price of a firm's own products (excluding current observation) in each year
  mutate(total_sum = sum(price),
         total_n = n()) %>% 
  ungroup %>% 
  group_by(Year,company) %>% 
  mutate(own_sum = sum(price),
         own_n = n()) %>% 
  ungroup %>% 
  mutate(comp_avg_price = (total_sum-own_sum)/(total_n-own_n),
         own_avg_price = (own_sum-price)/(own_n-1)
         ) %>%
  select(-c(total_sum,total_n,own_sum,own_n)) %>% 
  #the following generates an own price to average price in class ratio 
  group_by(Year,class) %>% 
  mutate(total_sum = sum(price),
         total_n = n()) %>%
  ungroup %>% 
  mutate(own_to_class_price_ratio = (total_sum-price)/(total_n-1)) %>% 
  select(-c(total_sum,total_n)) %>% 
  #remove columns that do not vary within each year
  select(-all_of(zero_var_in_year))

#Note that body_original and class do not agree:
distinct(df,body_original,class)

#class of columns
lapply(df,class)
  
```

<!-- Data Splitting -->
```{r Data_splitting, include=FALSE}
set.seed(123)

df_split <- initial_split(df, strata = price)

df_train <- training(df_split)
df_test <- testing(df_split)

df_folds <- bootstraps(df_train, strata = price)

doParallel::registerDoParallel()

```

<!-- Tune? -->
```{r To_tune_or_not_to_tune}
tune <- FALSE
```

<!-- LASSO -->
```{r LASSO_workflow}
#use_glmnet()

glmnet_recipe <- 
  recipe(formula = share ~ ., data = df_train) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors(), -all_nominal()) 

glmnet_spec <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") 

glmnet_workflow <- 
  workflow() %>% 
  add_recipe(glmnet_recipe) %>% 
  add_model(glmnet_spec) 


```

```{r LASSO_tune_fit_pred, eval = tune}

set.seed(63663)

glmnet_tune <- 
  tune_grid(glmnet_workflow, 
            resamples = df_folds, 
            grid = 10) 

show_best(glmnet_tune, metric = "rmse")
#Best model is penalty = 6.158482e-06


final_glmnet <- 
  glmnet_workflow %>%
  finalize_workflow(select_best(glmnet_tune, metric = "rmse"))
  
glmnet_fit <- last_fit(final_glmnet, df_split)

```

```{r LASSO_custom_fit_pred, eval = 1-tune}

final_glmnet <- 
  workflow() %>% 
  add_recipe(glmnet_recipe) %>% 
  add_model(update(glmnet_spec,
                   penalty=6.158482e-06)
            ) 

glmnet_fit <- last_fit(final_glmnet, df_split)

```

```{r LASSO_plot_pred_and_vip}

glmnet_plot <- #Plot of predictions vs truth
  collect_predictions(glmnet_fit) %>%
  ggplot(aes(share, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()

glmnet_vip <- 
  glmnet_fit %>% 
   pluck(".workflow", 1) %>%   
   pull_workflow_fit() %>% 
   vip::vip(num_features = 10)
```

<!-- CART -->
```{r CART_workflow}
#Preparing workflow
cart_recipe <- 
  recipe(formula = share ~ ., data = df_train) %>% 
  step_dummy(all_nominal())

cart_spec <- 
  decision_tree(cost_complexity = tune(), tree_depth=tune(), min_n = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("rpart") 

cart_workflow <- 
  workflow() %>% 
  add_recipe(cart_recipe) %>% 
  add_model(cart_spec) 

```

```{r CART_tune_fit_pred, eval = tune}

set.seed(63663)

cart_tune <-
  tune_grid(cart_workflow, 
            resamples = df_folds, 
            grid = 30,
            control = control_grid(verbose = TRUE))

show_best(cart_tune, metric = "rmse")
#Best model is cost_complexity = 3.162278e-06, tree_depth = 8, min_n = 40


final_cart <- cart_workflow %>%
  finalize_workflow(select_best(cart_tune, metric = "rmse"))

cart_fit <- last_fit(final_cart, df_split)

```

```{r CART_custom_fit_pred, eval = 1-tune}

final_cart <- 
  workflow() %>% 
  add_recipe(glmnet_recipe) %>% 
  add_model(update(cart_spec,
                   cost_complexity = 3.16227766016838e-06, tree_depth = 8, min_n = 40)
            ) 

cart_fit <- last_fit(final_cart, df_split)

```

```{r CART_plot_pred_and_vip}

cart_plot <- #Plot of predictions vs truth
  collect_predictions(cart_fit) %>%
  ggplot(aes(share, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()

cart_vip <- 
  cart_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)
```

<!-- Random Forest -->
```{r RF_workflow}
#Preparing workflow
#use_ranger(share ~ ., data = df_train)
rf_recipe <- 
  recipe(formula = share ~ ., data = df_train) %>% 
  step_dummy(all_nominal())

rf_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

rf_workflow <- 
  workflow() %>% 
  add_recipe(rf_recipe) %>% 
  add_model(rf_spec) 
```

```{r RF_tune_fit_pred, eval = tune}

set.seed(63663)

rf_tune <- tune_grid(rf_workflow, 
                     resamples = df_folds, 
                     grid = 20)

show_best(rf_tune, metric = "rmse")
#Best model is mtry = 10, min_n = 4


final_rf <- rf_workflow %>%
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

rf_fit <- last_fit(final_rf, df_split)

```

```{r RF_custom_fit_pred, eval = 1-tune}

final_rf <- 
  workflow() %>% 
  add_recipe(rf_recipe) %>% 
  add_model(update(rf_spec,
                   mtry = 10, min_n = 4, trees = 1000)
            )

rf_fit <- last_fit(final_rf, df_split)

```

```{r RF_plot_pred_and_vip}
rf_plot <- #Plot of predictions vs truth
  collect_predictions(rf_fit) %>%
  ggplot(aes(share, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()

rf_vip <- 
  rf_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)
```

<!-- XG-Boost -->
```{r XGB_workflow}
xgb_recipe <- 
  recipe(formula = share ~ ., data = df_train) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) 

xgb_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost") 

xgb_workflow <- 
  workflow() %>% 
  add_recipe(xgboost_recipe) %>% 
  add_model(xgboost_spec) 
```

```{r XGB_tune_fit_pred, eval = tune}

set.seed(82993)

xgb_tune <- tune_grid(xgboost_workflow, resamples = df_folds, grid = 60)

show_best(ranger_tune, metric = "rmse")

final_xgb <- xgb_workflow %>%
  finalize_workflow(select_best(xgb_tune, metric = "rmse"))

xgb_fit <- last_fit(final_xgb, df_split)

```

```{r XGB_custom_fit_pred, eval = 1-tune}

final_xgb <- 
  workflow() %>% 
  add_recipe(xgb_recipe) %>% 
  add_model(update(xgb_spec,
                   trees = , min_n = , tree_depth = , 
                   learn_rate = , loss_reduction = , sample_size = )
            )

xgb_fit <- last_fit(final_xgb, df_split)

```

```{r XGB_plot_pred_and_vip}

xgb_plot <- #Plot of predictions vs truth
  collect_predictions(xgb_fit) %>%
  ggplot(aes(share, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()

xgb_vip <- 
  xgb_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)
```


<!-- Collect and plot performance metrics of algorithms -->
```{r}
metrics_df <- 
  bind_rows(collect_metrics(glmnet_fit) %>% mutate(algo = "LASSO"),
            collect_metrics(cart_fit) %>% mutate(algo = "CART"),
            collect_metrics(rf_fit) %>% mutate(algo = "Random Forest"),
            collect_metrics(xgb_fit) %>% mutate(algo = "XG Boost"),
  )

metrics_plot <- 
  metrics_df %>% 
  ggplot(aes(algo,.estimate)) +
  geom_point() +
  facet_grid(.metric ~ ., scales = "free_y")
```

<!-- Collect and plot variable importance plots of algorithms -->
```{r}

```

