---
title: "Bagging and Trees"
description: |
  For learners with low bias and high variance
author:
  - name: Amit Gandhi
    url: {}
date: 02-08-2021
output:
  distill::distill_article:
    self_contained: false
preview: https://upload.wikimedia.org/wikipedia/commons/d/de/Ozone.png
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(ipred)       # for fitting bagged decision trees
library(rsample)
ames <- AmesHousing::make_ames()
```


# Introduction

We can think about an algorithmic model as being defined by a **learner** - an algorithm that fits training data given hyper-parameters that are subject to tuning. 

Decision trees are a class of learners. As a class they are generally characterized as **high variance** and **low bias**, e.g., they are very expressive and flexible (e.g., low bias) but as a result risk over-fitting the training data. 

Pruning is one approach to managing the high variance of decision trees. Unfortunately pruning destroys much of the expressiveness of the tree by forcing it to be coarse. Is there a better way to manage a low bias + high variance learner. 

In what follows below we borrow code and examples from the excellent book [Hands on Machine Learning with R](https://bradleyboehmke.github.io/HOML/)

## Visualizing the behavior of trees

Lets simulate some bivariate data

```{r}
# create data
set.seed(1112)  # for reproducibility
df <- tibble::tibble(
  x = seq(from = 0, to = 2 * pi, length = 500),
  y = sin(x) + rnorm(length(x), sd = 0.5),
  truth = sin(x)
)

```



Lets consider a single stump decision tree model that is fit by appropriately setting the control parameters in `rpart`

```{r}
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 1)
fit <- rpart(y ~ x, data = df, control = ctrl)
```

Lets plot the tree

```{r}
par(mar = c(1, 1, 1, 1))
rpart.plot(fit)
```


We can add the fitted values to the data

```{r}
df <- 
  df %>%
  mutate(pred = predict(fit, df))
```


```{r}
plot1 <-
  df %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = .75)

plot1
```

Plot the fitted tree against the data

```{r}
  plot1 + 
  
  geom_line(aes(y = pred), color = "red", size = .75) +
  geom_segment(x = 3.1, xend = 3.1, y = -Inf, yend = -.95,
               arrow = arrow(length = unit(0.25,"cm")), size = .25) +
  annotate("text", x = 3.1, y = -Inf, label = "split", hjust = 1.2, vjust = -1, size = 3) +
  geom_segment(x = 5.5, xend = 6, y = 2, yend = 2, size = .75, color = "blue") +
  geom_segment(x = 5.5, xend = 6, y = 1.7, yend = 1.7, size = .75, color = "red") +
  annotate("text", x = 5.3, y = 2, label = "truth", hjust = 1, size = 3, color = "blue") +
  annotate("text", x = 5.3, y = 1.7, label = "decision boundary", hjust = 1, size = 3, color = "red")
```





A less regularized (higher variance) tree is a learner that is allowed to grow large. 

```{r deep-overfit-tree, echo=TRUE, fig.width=4, fig.height=3, fig.show='hold', out.width="48%"}
ctrl <- list(cp = 0, minbucket = 1, maxdepth = 50)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)

```

```{r}
p1<- 
  df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = 0.75) +
  geom_line(aes(y = pred), color = "red", size = 0.75)

p1
```

The CART strategy is to grow large and prune. 


```{r pruned-tree, fig.width=10, fig.height = 4, fig.cap="To prune a tree, we grow an overly complex tree (left) and then use a cost complexity parameter to identify the optimal subtree (right).", echo=TRUE}

fit2 <- rpart(y ~ x, data = df)

p2 <- df %>%
  mutate(pred2 = predict(fit2, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred2), color = "red", size = 1)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

# How does bagging work and what does it acheive?

### Bootstrapping

The basis for bagging is resampling the training data via bootstrap sampling.

```{r modeling-process-bootstrapscheme, echo=TRUE, out.width='70%', out.height='70%', fig.cap="Illustration of the bootstrapping process."}
knitr::include_graphics("bootstrap-scheme.png")
```

We are not bootstrapping for the usual reason - to assess the uncertainty of a parameter estimate $\hat{\theta}$. 

Instead we are bootstrapping for the purposes of averaging across bootstrap samples, and hence the name *bag*gging where *bag* = **bootstrap aggregation**. 

### How does this work:

We can take repeated samples from the (single) training data set. 

We will generate $B$ different bootstrapped training data sets.

We then train our method on the $b$th bootstrapped training set in order to get $\hat{f}^{*b}(x).$ 

Finally, we average all the predictions, to obtain
$$\hat{f}_\text{bag}(x) = \frac{1}{B}
\sum_{b=1}^B \hat{f}^{*b}(x).$$

### Why does this work?

Consider a training sample $(x_1, y_1), \dots , (x_n, y_n)$ drawn from an underlying distribution $P$ that describes the joint distribution of $Y,X$.

Let us define a theoretical bagged quantity 

$$
f_{bag} = E_{P} \hat{f}(x)
$$

Here the expectation considers redrawing the training data from the population distribution $P$, and bootstrap sampling is a sample analogue of this experiment. 

Then we can express the prediction error

\begin{align}
E_{P}[(Y - \hat{f}(x)] &= E_{P}\left[ \left(Y - f_{agg}(x)\right)^2 \right] +  E_{P}\left[ \left(\hat{f}(x) - f_{agg}(x)\right)^2 \right] \\
&\geq E_{P}\left[ \left(Y - f_{agg}(x)\right)^2 \right]
\end{align}

Thus we can see bagging directly benefit from the variance reduction associated with elimination of the second term in the prediction error. 

The bootstrap has significant effects for high variance learners and hence payoff for low bias + high variance learners.


### Bagging for Regression Trees


To apply bagging to regression trees do the following: 

1. construct $B$ regression trees using $B$ bootstrapped training sets
2. average the resulting predictions. 

Regression trees are grown deep, and are not pruned. 

Hence each individual tree has high variance, but low bias. Thus, 
averaging these $B$ trees reduces the variance. 

Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.

### Bagging for Classification Trees

Bagging for Classification Trees
===

There are many ways to apply bagging for classification trees. 

We explain the simplest way. 


1. For a given test observation, we can record the class predicted by each of the $B$ trees.  

2. Take a majority vote. 

A majority vote is simply the overall prediction is the most commonly occurring class among the $B$ predictions.

This is the so-called **wisdom of crowd** effects. It requires starting with a weak learner. 

Bagging a bad learner for 0-1 classification loss can make prediction worse. See the example at the top of page 286 in ESL. 

### OOB Error Estimation

First consider visually how bootstrap sampling differs from cross validation

```{r modeling-process-sampling-comparison, echo=TRUE, fig.cap="Bootstrap sampling (left) versus 10-fold cross validation (right) on 32 observations. For bootstrap sampling, warning=FALSE, the observations that have zero replications (white) are the out-of-bag observations used for validation.", message=FALSE}
boots <- rsample::bootstraps(mtcars, 10)
boots_plot <- boots$splits %>%
  purrr::map2_dfr(seq_along(boots$splits), ~ mtcars %>% 
             mutate(
               Resample = paste0("Bootstrap_", stringr::str_pad(.y, 2, pad = 0)),
               ID = row_number()
             ) %>%
             group_by(ID) %>%
             mutate(Replicates = factor(sum(ID == .x$in_id)))) %>%
  ggplot(aes(Resample, ID, fill = Replicates)) +
  geom_tile() +
  scale_fill_manual(values = c("#FFFFFF", "#F5F5F5", "#C8C8C8", "#A0A0A0", "#707070", "#505050", "#000000")) +
  scale_y_reverse("Observation ID", breaks = 1:nrow(mtcars), expand = c(0, 0)) +
  scale_x_discrete(NULL, expand = c(0, 0)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Bootstrap sampling") 

cv <- vfold_cv(mtcars, 10)
cv_plot <- cv$splits %>%
  purrr::map2_dfr(seq_along(cv$splits), ~ mtcars %>% mutate(
    Resample = paste0("Fold_", stringr::str_pad(.y, 2, pad = 0)),
    ID = row_number(),
    Data = ifelse(ID %in% .x$in_id, "Training", "Validation"))
    ) %>%
  ggplot(aes(Resample, ID, fill = Data)) +
  geom_tile() +
  scale_fill_manual(values = c("#f2f2f2", "#AAAAAA")) +
  scale_y_reverse("Observation ID", breaks = 1:nrow(mtcars), expand = c(0, 0)) +
  scale_x_discrete(NULL, expand = c(0, 0)) +
  theme_classic() +
  theme(legend.title=element_blank())



cv_plot <- cv_plot + 
  ggtitle("10-fold cross validation") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

cowplot::plot_grid(boots_plot, cv_plot, align = "h", nrow = 1)

# clean up
rm(boots, boots_plot, cv_plot)
```


The sampling scheme reveals an important insight - the OOB (*Out-of-Bag*) experiment.

One can show that on average, each bagged tree makes use of around two-thirds of the observations (this is evident from the white cells across any row in the bootstrap graph above)

The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.

We can predict the response for the $i$th observation using each of the trees in which that observation was OOB.

Thus in order to obtain a single prediction for the $i$th observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal).

This leads to a single OOB prediction for the ith observation.

An OOB prediction can be obtained in this way for each of the $n$  observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. 

The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.

It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. 

The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.

### A simulated bagging experiment

```{r bagging-multiple-models, echo=TRUE, fig.width=12, fig.cap="The effect of bagging 100 base learners. High variance models such as decision trees (B) benefit the most from the aggregation effect in bagging, whereas low variance models such as polynomial regression (A) show little improvement. "}
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

# bootstrapped polynomial model fit
bootstrap_n <- 100
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  # reproducible sampled data frames
  set.seed(i)
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  
  # fit model and add predictions to results data frame
  fit <- lm(y ~ I(x^3), data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}

p1 <- ggplot(bootstrap_results, aes(x, predictions)) +
  geom_point(data = df, aes(x, y), alpha = .25) +
  geom_line(aes(group = model), show.legend = FALSE, size = .5, alpha = .2) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "line") +
  scale_y_continuous("Response", limits = c(-2, 2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) +
  ggtitle("A) Polynomial regression")



# bootstrapped decision trees fit
bootstrap_n <- 100
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  # reproducible sampled data frames
  set.seed(i)
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  
  # fit model and add predictions to results data frame
  fit <- rpart::rpart(y ~ x, data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}

p3 <- ggplot(bootstrap_results, aes(x, predictions)) +
  geom_point(data = df, aes(x, y), alpha = .25) +
  geom_line(aes(group = model), show.legend = FALSE, size = .5, alpha = .2) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "line") +
  scale_y_continuous(NULL, limits = c(-2, 2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) +
  ggtitle("B) Decision trees")

gridExtra::grid.arrange(p1, p3, nrow = 1)
```


### Revisiting the Ames, IA data

Lets try bagging on the Ames, IA data. First we setup the data as we did in prior posts. 

```{r 08-ames-train, echo=TRUE}
# create Ames training data
set.seed(123)
ames <- AmesHousing::make_ames()
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test  <- testing(split)
```


We will use the `bagging()` function from the `ipred` package. 


```{r first-bagged-ames-model}
# make bootstrapping reproducible
set.seed(123)

# train bagged model
ames_bag1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag1
```

How does this compare to pruning a decision tree. With enough trees, the performance of bagging exceeds that of pruning. 

```{r n-bags-plot, echo=TRUE, fig.cap="Error curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees."}
# # assess 10-200 bagged trees
# ntree <- seq(10, 200, by = 2)
# 
# # create empty vector to store OOB RMSE values
# rmse <- vector(mode = "numeric", length = length(ntree))
# 
# for (i in seq_along(ntree)) {
#   # reproducibility
#   set.seed(123)
#   # perform bagged model
#   model <- bagging(
#   formula = Sale_Price ~ .,
#   data    = ames_train,
#   coob    = TRUE,
#   control = rpart.control(minsplit = 2, cp = 0),
#   nbagg   = ntree[i]
# )
#   # get OOB error
#   rmse[i] <- model$err
# }
# 
# bagging_errors <- data.frame(ntree, rmse)

# using ranger to do the same as above.  Will allow for bagging under 10 trees
# and is much faster!
ntree <- seq(1, 200, by = 2)
# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(123)
  # perform bagged model
  model <- ranger::ranger(
  formula = Sale_Price ~ .,
  data    = ames_train,
  num.trees = ntree[i],
  mtry = ncol(ames_train) - 1,
  min.node.size = 1
)
  # get OOB error
  rmse[i] <- sqrt(model$prediction.error)
}

bagging_errors <- data.frame(ntree, rmse)

ggplot(bagging_errors, aes(ntree, rmse)) +
  geom_line() +
  geom_hline(yintercept = 41019, lty = "dashed", color = "grey50") +
  annotate("text", x = 100, y = 41385, label = "Best individual pruned tree", vjust = 0, hjust = 0, color = "grey50") +
  annotate("text", x = 100, y = 26750, label = "Bagged trees", vjust = 0, hjust = 0) +
  ylab("RMSE") +
  xlab("Number of trees")
```